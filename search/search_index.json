{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to our playbook. This document aims help our engineering, product, and design teams to operate within a clear framework of practices and achieve our mission . Who is this for? \u00b6 This is for anyone in our team, anyone working with our team with an interest in how we work, anyone thinking about working in our team, or anyone with a passing interest in our values. We have made this document public, so that we can be held to the standards set out. Do's and Don'ts \u00b6 DO \u00b6 CONTRIBUTE - this is a public document, and is owned by the whole department. We welcome comment form external readers. Team members should submit pull requests where they identify potential improvements, and collaborate with their team members to refine processes and standards. Details of how to contribute are here . READ ME - it's not just here to fill up space on the giant virtual bookshelf DO NOT \u00b6 PUT SECRETS HERE - No secrets, passwords, or sensitive information. If you wouldn't give it to a baddy, don't put it here. TROLL - Sometimes we get things wrong. That's OK. If you want to criticise that is great, but it had better be helpful. Join Our Team \u00b6 Current opportunities in Software Engineering, Design, and Product management: And take a look at our careers site for more information about life at AM.","title":"Home"},{"location":"#who-is-this-for","text":"This is for anyone in our team, anyone working with our team with an interest in how we work, anyone thinking about working in our team, or anyone with a passing interest in our values. We have made this document public, so that we can be held to the standards set out.","title":"Who is this for?"},{"location":"#dos-and-donts","text":"","title":"Do's and Don'ts"},{"location":"#do","text":"CONTRIBUTE - this is a public document, and is owned by the whole department. We welcome comment form external readers. Team members should submit pull requests where they identify potential improvements, and collaborate with their team members to refine processes and standards. Details of how to contribute are here . READ ME - it's not just here to fill up space on the giant virtual bookshelf","title":"DO"},{"location":"#do-not","text":"PUT SECRETS HERE - No secrets, passwords, or sensitive information. If you wouldn't give it to a baddy, don't put it here. TROLL - Sometimes we get things wrong. That's OK. If you want to criticise that is great, but it had better be helpful.","title":"DO NOT"},{"location":"#join-our-team","text":"Current opportunities in Software Engineering, Design, and Product management: And take a look at our careers site for more information about life at AM.","title":"Join Our Team"},{"location":"1.-Welcome/Mission/","text":"Ensure the fast, predictable, and uninterrupted flow of planned work That delivers value to the business By delivering stable and secure software, and minimising the impact of unplanned work.","title":"Mission"},{"location":"1.-Welcome/Onboarding/","text":"Who is this for? \u00b6 This is the place to start when you join Platform Development at AM. It is designed for new starters, regardless of their role. It will point you to key areas of the playbook, to provide a quick overview of the department and how we work. Much of this should be covered during your induction. The playbook is your companion from induction to mastery. Initial Reading \u00b6 The Platform Development Division have a unified mission . Everything we do aims to aid us in achieving this mission. And we operate within an end to end delivery framework Platform Development is organised into a set of teams The delivery teams operate in a sprint model following the Scrum framework We build and maintain the Quartex , internal tooling, and legacy platforms. Check out the Knowledge Base in Azure DevOps and the Roadmaps. Besides our framework of practices, the way we collaborate and communicate with each other is critical. Read the Collaboration & Ops Development Practices \u00b6 This section will be most relevant to Software Engineers, but all team members are encouraged to read. // TODO Testing Practices \u00b6 This section will be most relevant to Test Engineers, but all team members are encouraged to read. Quality is the responsibility of the whole team Where appropriate, new features and changes are tested on an independent environment against Acceptance Criteria / BDD Scenarios before releasing to live environments We aim to follow a TDD approach to minimise defects/bugs and ensure software can be continuously released without issue End-to-End Automation test scripts for Quartex are written in Playwright (JavaScript) Exploratory Testing compliments these practices, using Test Charters where appropriate Releasing Practices \u00b6 Business Acceptance Testing (BAT) is performed for larger features before releasing to live environments The release of features and changes to live environments are planned within the development team delivering the work item Releases to live environments occur throughout the sprint and are coordinated between teams Product Management & Design Practices \u00b6 // TODO Tools and Access \u00b6 Tooling and platform directories live in the Knowledge Base. Therein you should find details of the tools and accounts you will likely require access to.","title":"Onboarding"},{"location":"1.-Welcome/Onboarding/#who-is-this-for","text":"This is the place to start when you join Platform Development at AM. It is designed for new starters, regardless of their role. It will point you to key areas of the playbook, to provide a quick overview of the department and how we work. Much of this should be covered during your induction. The playbook is your companion from induction to mastery.","title":"Who is this for?"},{"location":"1.-Welcome/Onboarding/#initial-reading","text":"The Platform Development Division have a unified mission . Everything we do aims to aid us in achieving this mission. And we operate within an end to end delivery framework Platform Development is organised into a set of teams The delivery teams operate in a sprint model following the Scrum framework We build and maintain the Quartex , internal tooling, and legacy platforms. Check out the Knowledge Base in Azure DevOps and the Roadmaps. Besides our framework of practices, the way we collaborate and communicate with each other is critical. Read the Collaboration & Ops","title":"Initial Reading"},{"location":"1.-Welcome/Onboarding/#development-practices","text":"This section will be most relevant to Software Engineers, but all team members are encouraged to read. // TODO","title":"Development Practices"},{"location":"1.-Welcome/Onboarding/#testing-practices","text":"This section will be most relevant to Test Engineers, but all team members are encouraged to read. Quality is the responsibility of the whole team Where appropriate, new features and changes are tested on an independent environment against Acceptance Criteria / BDD Scenarios before releasing to live environments We aim to follow a TDD approach to minimise defects/bugs and ensure software can be continuously released without issue End-to-End Automation test scripts for Quartex are written in Playwright (JavaScript) Exploratory Testing compliments these practices, using Test Charters where appropriate","title":"Testing Practices"},{"location":"1.-Welcome/Onboarding/#releasing-practices","text":"Business Acceptance Testing (BAT) is performed for larger features before releasing to live environments The release of features and changes to live environments are planned within the development team delivering the work item Releases to live environments occur throughout the sprint and are coordinated between teams","title":"Releasing Practices"},{"location":"1.-Welcome/Onboarding/#product-management-design-practices","text":"// TODO","title":"Product Management &amp; Design Practices"},{"location":"1.-Welcome/Onboarding/#tools-and-access","text":"Tooling and platform directories live in the Knowledge Base. Therein you should find details of the tools and accounts you will likely require access to.","title":"Tools and Access"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/","text":"General Guidelines for Documentation \u00b6 Including the correct information and putting it in the right place \u00b6 Firstly, review the section on Documentation Types, and follow those guidelines! Generally, some documentation is better than no documentation. It doesn't need to be perfect or complete straight away! Always include information about when the document was first written AND last reviewed. Knowing the correct audience \u00b6 Include, at the start, who the target audience is, and write documentation that will be understood by that audience. Consider the value that this audience will gain from it, and summarise this as the purpose of the document. If there is a need for technical and non-technical readers, consider breaking it up. How to keep it up to date \u00b6 Documentation should be reviewed periodically as most types of documentation have a fairly short lifespan. It is important that we know when we will next review each document, as such this should be determined when a document is published and when it is reviewed or updated. Obviously this does not apply to all types of document - such as code and commit comments. Repository readmes should always be reviewed and updated when related code is changed Ownership \u00b6 The creator / reviewer is responsible for determining the next review date Everyone is responsible for ensuring documentation remains up to date. Reviewing \u00b6 Documentation Types \u00b6 We can perceive documentation as having different \"levels\" . From high level documentation covering ways of working with the widest audience, to lowest level documentation covering the function of modules or even lines of code. Within each level there are a number of types of documentation. Specific guidelines for each of these levels and their types are detailed in the subpages of this section of the wiki. Level Types Locations Department & Team Level Process and practice guidance, standards, onboarding The Platform Development Playbook Platform Level Architecture, infrastructure, patterns, developer guides Knowledge Base, Non-functional requirements Business Value & Functional level Functional requirements, Strategy, Roadmap items Functional guides, Roadmaps, Backlog Items & Feature Files Solution/Repository Level Solution overview, Solution-specific architecture & coding patterns, Build & run guides, Dependencies and known issues, Change & release logs ReadMe files, Code commit messages, Release plans, Knowledge Base Interface Level UI user guides, API developer guides User guides, Endpoint comments/Swagger(etc.) API documentation Code Level Code comments Classes and methods","title":"Documentation Guidelines"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#general-guidelines-for-documentation","text":"","title":"General Guidelines for Documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#including-the-correct-information-and-putting-it-in-the-right-place","text":"Firstly, review the section on Documentation Types, and follow those guidelines! Generally, some documentation is better than no documentation. It doesn't need to be perfect or complete straight away! Always include information about when the document was first written AND last reviewed.","title":"Including the correct information and putting it in the right place"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#knowing-the-correct-audience","text":"Include, at the start, who the target audience is, and write documentation that will be understood by that audience. Consider the value that this audience will gain from it, and summarise this as the purpose of the document. If there is a need for technical and non-technical readers, consider breaking it up.","title":"Knowing the correct audience"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#how-to-keep-it-up-to-date","text":"Documentation should be reviewed periodically as most types of documentation have a fairly short lifespan. It is important that we know when we will next review each document, as such this should be determined when a document is published and when it is reviewed or updated. Obviously this does not apply to all types of document - such as code and commit comments. Repository readmes should always be reviewed and updated when related code is changed","title":"How to keep it up to date"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#ownership","text":"The creator / reviewer is responsible for determining the next review date Everyone is responsible for ensuring documentation remains up to date.","title":"Ownership"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#reviewing","text":"","title":"Reviewing"},{"location":"1.-Welcome/Documentation-Guidelines/Documentation-Guidelines/#documentation-types","text":"We can perceive documentation as having different \"levels\" . From high level documentation covering ways of working with the widest audience, to lowest level documentation covering the function of modules or even lines of code. Within each level there are a number of types of documentation. Specific guidelines for each of these levels and their types are detailed in the subpages of this section of the wiki. Level Types Locations Department & Team Level Process and practice guidance, standards, onboarding The Platform Development Playbook Platform Level Architecture, infrastructure, patterns, developer guides Knowledge Base, Non-functional requirements Business Value & Functional level Functional requirements, Strategy, Roadmap items Functional guides, Roadmaps, Backlog Items & Feature Files Solution/Repository Level Solution overview, Solution-specific architecture & coding patterns, Build & run guides, Dependencies and known issues, Change & release logs ReadMe files, Code commit messages, Release plans, Knowledge Base Interface Level UI user guides, API developer guides User guides, Endpoint comments/Swagger(etc.) API documentation Code Level Code comments Classes and methods","title":"Documentation Types"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/","text":"Who is this for? \u00b6 This playbook outlines the continually evolving frameworks, processes, and practices that are used by the Platform Development Division to enable us to reliably deliver against our collective mission . All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support. Ownership and Responsibility \u00b6 Every member of the division is responsible for ensuring that this playbook is up to date and accurate. Ownership When you create, review, or modify a document, you are responsible for ensuring that it is well written and has been peer reviewed , and that review date is set . Tooling \u00b6 This playbook is build using Markdown documents, a simple text editor designed for documentation. Specifically, it is build on MKDocs , using the Material for MKDocs theme. It is stored in GitHub, and published with GitHub Pages MKDocs and Material for MKDocs extend the functionality of Markdown, allowing you to include visually richer content. You can edit the documents using any markdown editor, including GitHub. Read How to Contribute for a step by step guide. Structure \u00b6 Page header \u00b6 You must always provide a page header , using the following YAML heading set at the very top of each document in the playbook. --- title: [Fascinating subject] authors: - [Given name] [Family name] - [John] [McWriter] reviewed: [yyyy-mm-dd] reviewer: next-review: --- Note Dates must be formatted yyyy-mm-dd (e.g. 2022-08-31, following ISO 8601 ) You can insert as many authors required, each author on a new line. Please make sure you use full name The only exception if child pages, when they are used specifically for breaking down larger subjects, where the authors are the same, and where they would naturally be reviewed alongside the parent. This this instance you must still include a title. --- title: --- Headings \u00b6 The main heading (e.g. H1) for each page is automatically generated from the title field. Therefore you should not add any top level headings to the page content Use structured headings starting at H2 using ##. For subheading, increment the hashes (### for H3, ### for H4). ## First section Lorum ipsum UI pipeline, or herding cats, blue sky thinking, yet we need to leverage our synergies. ## Second section Lorum ipsum enough to wash your face, put a record on and see who dances. Bob called an all-hands this afternoon offline this discussion deliverables for deep dive but land it in region. ### First child of second section Lorum ipsum who's responsible for the ask for this request? run it up the flag pole so disband the squad but rehydrate as needed. Table of contents The table of contents is automatically generated, but will always exclude top level headings (i.e. H1). This is why you should start at H2! See the one on this page for example! General tips \u00b6 Start by explaining who and what the document is for Use sub pages if your content is large Use bullet lists to simplify key points Use diagrams to express more complex ideas, such as processes Use a spell checker in whatever editor you are using, set it to UK English. In VSCode there numerous plugins. Reviewing \u00b6 Documents should be peer reviewed after initial publication, and following significant revisions. They should also be subject to periodic reviews to ensure they are up to date, relevant, and accurate. Peer Reviewing \u00b6 The primary purpose of a review is to ensure that the documentation is as useful as it can be. The peer reviewer should carefully read and understand the content of the document. Consider the audience of the document and its purpose. Ensure the page header is populated and accurate, and that a review date has been provided. Check for typos and errors if possible, although this is not the primary purpose of review. Use a spell checker. Provide constructive feedback to the author for revision. Periodic Reviews \u00b6 On or around the review date, the document should be checked for validity. The purpose of a review is to ensure that the document is still as useful as it can be, or deleted if it is no longer required. Updates may happen at any time, but it is possible that changes to frameworks, processes, and practices have happened and not been documented. After a review, ensure that the Next Review Date in the page header is updated, regardless of whether any changes have been made. If a major revision is performed, add the name of the revising author(s) to the page header, and ensure that the revision is peer reviewed. How to Contribute \u00b6 // TODO: Create contributing guide, including tooling and submitting pull requests Oh no! This bit hasn't been done yet! Perhaps you could do it?","title":"Playbook Documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#who-is-this-for","text":"This playbook outlines the continually evolving frameworks, processes, and practices that are used by the Platform Development Division to enable us to reliably deliver against our collective mission . All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support.","title":"Who is this for?"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#ownership-and-responsibility","text":"Every member of the division is responsible for ensuring that this playbook is up to date and accurate. Ownership When you create, review, or modify a document, you are responsible for ensuring that it is well written and has been peer reviewed , and that review date is set .","title":"Ownership and Responsibility"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#tooling","text":"This playbook is build using Markdown documents, a simple text editor designed for documentation. Specifically, it is build on MKDocs , using the Material for MKDocs theme. It is stored in GitHub, and published with GitHub Pages MKDocs and Material for MKDocs extend the functionality of Markdown, allowing you to include visually richer content. You can edit the documents using any markdown editor, including GitHub. Read How to Contribute for a step by step guide.","title":"Tooling"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#structure","text":"","title":"Structure"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#page-header","text":"You must always provide a page header , using the following YAML heading set at the very top of each document in the playbook. --- title: [Fascinating subject] authors: - [Given name] [Family name] - [John] [McWriter] reviewed: [yyyy-mm-dd] reviewer: next-review: --- Note Dates must be formatted yyyy-mm-dd (e.g. 2022-08-31, following ISO 8601 ) You can insert as many authors required, each author on a new line. Please make sure you use full name The only exception if child pages, when they are used specifically for breaking down larger subjects, where the authors are the same, and where they would naturally be reviewed alongside the parent. This this instance you must still include a title. --- title: ---","title":"Page header"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#headings","text":"The main heading (e.g. H1) for each page is automatically generated from the title field. Therefore you should not add any top level headings to the page content Use structured headings starting at H2 using ##. For subheading, increment the hashes (### for H3, ### for H4). ## First section Lorum ipsum UI pipeline, or herding cats, blue sky thinking, yet we need to leverage our synergies. ## Second section Lorum ipsum enough to wash your face, put a record on and see who dances. Bob called an all-hands this afternoon offline this discussion deliverables for deep dive but land it in region. ### First child of second section Lorum ipsum who's responsible for the ask for this request? run it up the flag pole so disband the squad but rehydrate as needed. Table of contents The table of contents is automatically generated, but will always exclude top level headings (i.e. H1). This is why you should start at H2! See the one on this page for example!","title":"Headings"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#general-tips","text":"Start by explaining who and what the document is for Use sub pages if your content is large Use bullet lists to simplify key points Use diagrams to express more complex ideas, such as processes Use a spell checker in whatever editor you are using, set it to UK English. In VSCode there numerous plugins.","title":"General tips"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#reviewing","text":"Documents should be peer reviewed after initial publication, and following significant revisions. They should also be subject to periodic reviews to ensure they are up to date, relevant, and accurate.","title":"Reviewing"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#peer-reviewing","text":"The primary purpose of a review is to ensure that the documentation is as useful as it can be. The peer reviewer should carefully read and understand the content of the document. Consider the audience of the document and its purpose. Ensure the page header is populated and accurate, and that a review date has been provided. Check for typos and errors if possible, although this is not the primary purpose of review. Use a spell checker. Provide constructive feedback to the author for revision.","title":"Peer Reviewing"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#periodic-reviews","text":"On or around the review date, the document should be checked for validity. The purpose of a review is to ensure that the document is still as useful as it can be, or deleted if it is no longer required. Updates may happen at any time, but it is possible that changes to frameworks, processes, and practices have happened and not been documented. After a review, ensure that the Next Review Date in the page header is updated, regardless of whether any changes have been made. If a major revision is performed, add the name of the revising author(s) to the page header, and ensure that the revision is peer reviewed.","title":"Periodic Reviews"},{"location":"1.-Welcome/Documentation-Guidelines/Platform-Development-Playbook/#how-to-contribute","text":"// TODO: Create contributing guide, including tooling and submitting pull requests Oh no! This bit hasn't been done yet! Perhaps you could do it?","title":"How to Contribute"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/API-Documentation/","text":"","title":"API Documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Backlogs-and-requirements/","text":"","title":"Backlogs and requirements"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Comments-in-commits/","text":"","title":"Comments in commits"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Comments-within-code/","text":"","title":"Comments within code"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/","text":"README files \u00b6 Every repository must have a file called README.md in the root folder. Using markdown files instead of a plain text file allows us to format documentation, add links and images to enrich the documentation. As an added bonus, they get rendered nicely when browsing the repository via the GitHub web UI. Why include a README at all? \u00b6 Every repo must include a README file so that viewers know what the code does and how they can get started using it. Most importantly a README should serve to significantly reduce or remove friction when a new engineer starts working with a repository, so that they can start being productive as quickly as possible. Required sections \u00b6 Note The layout for our README files is based loosely on the Best README Template project. The Azure DevOps README guidelines are also a useful reference. Every README file must have the following sections: Overview Built with Getting started Usage Contributing Whilst these sections are a minimum, and must appear on the README for every repository we manage, they don't need to be lengthy! A brief summary or link out to existing documentation elsewhere is acceptable. Overview \u00b6 Provide a brief overview of the repository, and the problem it aims to solve. Built with \u00b6 List the major technologies and frameworks used within the repository, as a bullet-point list. Ideally link out to some documentation for each item, this is especially important for technologies and frameworks outside our usual tech stack. Getting started \u00b6 This section must clearly describe how someone who has just downloaded the repository can get started running/using it. It must include any pre-requisites or frameworks that must be installed locally, and any additional installation steps where relevant. Never make any assumptions about the engineer's local environment or tools. Make sure the instructions are step-by-step Usage \u00b6 Use this space to show useful examples of how a repository can be used. Additional screenshots, code examples and demos work well in this space. You may also link to more resources. The exact contents here will be different depending on the type of repository in question. A microservice repository must document the endpoints it implements, whilst a shared code repository must explain how to include any packages/modules and give examples of how to use the classes/libraries exposed. Contributing \u00b6 How should an engineer who wishes to contribute to the repository do so? In most cases, a link out to the relevant branching strategy will usually be sufficient. Template \u00b6 If starting from a blank slate, you could use this template as a starting point for a new README.md file. # ACME project ## Overview The ACME project is a thing that does stuff. ## Built with This repository is built using the following technologies: - technology - etc ## Getting started How does a new user get up-and-running with the project? What dependencies do they need to have installed? ## Usage Describe how to use the application/project under standard use-cases. ## Contributing How should an engineer get started contributing to the project.","title":"README files in code repositories"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#readme-files","text":"Every repository must have a file called README.md in the root folder. Using markdown files instead of a plain text file allows us to format documentation, add links and images to enrich the documentation. As an added bonus, they get rendered nicely when browsing the repository via the GitHub web UI.","title":"README files"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#why-include-a-readme-at-all","text":"Every repo must include a README file so that viewers know what the code does and how they can get started using it. Most importantly a README should serve to significantly reduce or remove friction when a new engineer starts working with a repository, so that they can start being productive as quickly as possible.","title":"Why include a README at all?"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#required-sections","text":"Note The layout for our README files is based loosely on the Best README Template project. The Azure DevOps README guidelines are also a useful reference. Every README file must have the following sections: Overview Built with Getting started Usage Contributing Whilst these sections are a minimum, and must appear on the README for every repository we manage, they don't need to be lengthy! A brief summary or link out to existing documentation elsewhere is acceptable.","title":"Required sections"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#overview","text":"Provide a brief overview of the repository, and the problem it aims to solve.","title":"Overview"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#built-with","text":"List the major technologies and frameworks used within the repository, as a bullet-point list. Ideally link out to some documentation for each item, this is especially important for technologies and frameworks outside our usual tech stack.","title":"Built with"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#getting-started","text":"This section must clearly describe how someone who has just downloaded the repository can get started running/using it. It must include any pre-requisites or frameworks that must be installed locally, and any additional installation steps where relevant. Never make any assumptions about the engineer's local environment or tools. Make sure the instructions are step-by-step","title":"Getting started"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#usage","text":"Use this space to show useful examples of how a repository can be used. Additional screenshots, code examples and demos work well in this space. You may also link to more resources. The exact contents here will be different depending on the type of repository in question. A microservice repository must document the endpoints it implements, whilst a shared code repository must explain how to include any packages/modules and give examples of how to use the classes/libraries exposed.","title":"Usage"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#contributing","text":"How should an engineer who wishes to contribute to the repository do so? In most cases, a link out to the relevant branching strategy will usually be sufficient.","title":"Contributing"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Readme-files-in-code-repositories/#template","text":"If starting from a blank slate, you could use this template as a starting point for a new README.md file. # ACME project ## Overview The ACME project is a thing that does stuff. ## Built with This repository is built using the following technologies: - technology - etc ## Getting started How does a new user get up-and-running with the project? What dependencies do they need to have installed? ## Usage Describe how to use the application/project under standard use-cases. ## Contributing How should an engineer get started contributing to the project.","title":"Template"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/Release-documentation/","text":"","title":"Release documentation"},{"location":"1.-Welcome/Documentation-Guidelines/Other%20Documentation%20Types/User-Guides/","text":"","title":"User Guides"},{"location":"2.-Delivery-Framework/","text":"Purpose \u00b6 Provide an overview of the delivery framework and its stages and their purpose, including inputs, outputs and guidelines for success. Audience \u00b6 All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support. This may also be of interest to any non-technical stakeholder from across the business. Overview \u00b6 Our agile Delivery Framework is designed as a robust and clear abstract mechanism by which value is derived, delivered, and maintained by the organisation. While the high-level framework itself is fixed, the processes and practices that live within it should be constantly evolving, such that the framework continues to support our mission . You can see the detailed view of each of the process states by viewing the Miro Board","title":"Delivery Framework"},{"location":"2.-Delivery-Framework/#purpose","text":"Provide an overview of the delivery framework and its stages and their purpose, including inputs, outputs and guidelines for success.","title":"Purpose"},{"location":"2.-Delivery-Framework/#audience","text":"All members of the Platform Development Division - Product Management, Software Engineering, Test Engineering, Platform Support. This may also be of interest to any non-technical stakeholder from across the business.","title":"Audience"},{"location":"2.-Delivery-Framework/#overview","text":"Our agile Delivery Framework is designed as a robust and clear abstract mechanism by which value is derived, delivered, and maintained by the organisation. While the high-level framework itself is fixed, the processes and practices that live within it should be constantly evolving, such that the framework continues to support our mission . You can see the detailed view of each of the process states by viewing the Miro Board","title":"Overview"},{"location":"2.-Delivery-Framework/Topology-of-Business-Value/","text":"The Types of Backlog Item \u00b6 Software delivery is more than building features or working through a backlog of \"user stories\". Instead, we can should consider that everything we build ties directly to creating business value . `// TODO: Add in topology diagram, rationale and context","title":"Topology of Business Value"},{"location":"2.-Delivery-Framework/Topology-of-Business-Value/#the-types-of-backlog-item","text":"Software delivery is more than building features or working through a backlog of \"user stories\". Instead, we can should consider that everything we build ties directly to creating business value . `// TODO: Add in topology diagram, rationale and context","title":"The Types of Backlog Item"},{"location":"3.-Sprints-%26-Teams/","text":"Who is this for? \u00b6 This section and its subpages outline the \"Scrum\" delivery model, including outlining the structures, mechanisms and responsibilities therein. This should be read and understood by all members of the Platform Development department, not just the Product Delivery Team members. Introduction \u00b6 Currently we use the Scrum Framework to enable agile delivery. The documentation here is designed to extend and slightly adapt the core Scrum framework of practices to best suit our mission. You should therefore be already familiar with Scrum and it is recommended that you read the Scrum Guide . You should also have a solid understanding of Agile and DevOps theory and the value they can bring, this will be highly advantageous for the success of your team and the department as a whole. Scrum is not Agile, and Agile is not Scrum. They are distinct. Scrum outlines an approach to working in an Agile way, but being Agile requires an Agile mindset, which requires an understanding of its core principles and value proposition. The department currently is structured into two team types, including multiple Product Delivery Teams and a single Enablement Team Sprint Model \u00b6 The sprint includes a series of events that occur at the same time each day/sprint. Besides the key sprint events, there are other events that may occur periodically: - to support the design & definition of future work - currently as Backlog Refinement (although this will be reviewed as the new 3 Amigos practice is implemented) - to plan the runway of work for near future sprints - Runway planning Guidance and details on these events can be found in subpages. Product Delivery Teams \u00b6 These are small, cross functional, self organised \"Scrum teams\" that design, define, build, test, and release product increments that bring value to the business. These are described as product backlog items . Each team will consist of software engineers, test engineers, and a product manager. Product Delivery Teams (short hand: \"delivery teams\") will engage in both \"Sprint work\"- i.e. building, testing, and releasing product increments - and also preparation work - i.e. the design and definition (aka refinement) of future sprint work. Product Delivery Team Responsibilities \u00b6 Each of these teams has specific responsibilities that align with our mission : Sprint Success Determining which of the sprint Candidates can be achieved during the sprint, given the available resources and work that needs to be done (considering the Definition of Done), and communicating this to the Enablement Team Planning how this work will be done, including determining what work needs to be done and devising a strategy for completing it Identify blockers and risks that may prevent sprint success, and promptly communicating them to the Enablement Team Executing the planned work to the quality standards outlined in the Playbook Updating the plan as required, and communicating updates to the Enablement Team Planning and performing releases, and collaborating with the enablement team and other product delivery teams to minimise risk Measuring and reporting on the sprint progress Continued improvement of sprint effectiveness Reflecting upon and reporting on team effectiveness Identifying and implementing initiatives that help improve our overall effectiveness Measuring and reporting on the outcome of such initiatives, positive or negative Delivery teams must communicate the following information to the wider department (i.e. other delivery teams, stakeholders etc) Confirmation of sprint backlog (items and sprint priority) and goal following sprint planning Details of blockers and risks as and when they arise Sprint progress Changes to the sprint backlog should the plan needs to be updated Overview of sprint outcomes, including stories and total points delivered","title":"Sprints & Teams"},{"location":"3.-Sprints-%26-Teams/#who-is-this-for","text":"This section and its subpages outline the \"Scrum\" delivery model, including outlining the structures, mechanisms and responsibilities therein. This should be read and understood by all members of the Platform Development department, not just the Product Delivery Team members.","title":"Who is this for?"},{"location":"3.-Sprints-%26-Teams/#introduction","text":"Currently we use the Scrum Framework to enable agile delivery. The documentation here is designed to extend and slightly adapt the core Scrum framework of practices to best suit our mission. You should therefore be already familiar with Scrum and it is recommended that you read the Scrum Guide . You should also have a solid understanding of Agile and DevOps theory and the value they can bring, this will be highly advantageous for the success of your team and the department as a whole. Scrum is not Agile, and Agile is not Scrum. They are distinct. Scrum outlines an approach to working in an Agile way, but being Agile requires an Agile mindset, which requires an understanding of its core principles and value proposition. The department currently is structured into two team types, including multiple Product Delivery Teams and a single Enablement Team","title":"Introduction"},{"location":"3.-Sprints-%26-Teams/#sprint-model","text":"The sprint includes a series of events that occur at the same time each day/sprint. Besides the key sprint events, there are other events that may occur periodically: - to support the design & definition of future work - currently as Backlog Refinement (although this will be reviewed as the new 3 Amigos practice is implemented) - to plan the runway of work for near future sprints - Runway planning Guidance and details on these events can be found in subpages.","title":"Sprint Model"},{"location":"3.-Sprints-%26-Teams/#product-delivery-teams","text":"These are small, cross functional, self organised \"Scrum teams\" that design, define, build, test, and release product increments that bring value to the business. These are described as product backlog items . Each team will consist of software engineers, test engineers, and a product manager. Product Delivery Teams (short hand: \"delivery teams\") will engage in both \"Sprint work\"- i.e. building, testing, and releasing product increments - and also preparation work - i.e. the design and definition (aka refinement) of future sprint work.","title":"Product Delivery Teams"},{"location":"3.-Sprints-%26-Teams/#product-delivery-team-responsibilities","text":"Each of these teams has specific responsibilities that align with our mission : Sprint Success Determining which of the sprint Candidates can be achieved during the sprint, given the available resources and work that needs to be done (considering the Definition of Done), and communicating this to the Enablement Team Planning how this work will be done, including determining what work needs to be done and devising a strategy for completing it Identify blockers and risks that may prevent sprint success, and promptly communicating them to the Enablement Team Executing the planned work to the quality standards outlined in the Playbook Updating the plan as required, and communicating updates to the Enablement Team Planning and performing releases, and collaborating with the enablement team and other product delivery teams to minimise risk Measuring and reporting on the sprint progress Continued improvement of sprint effectiveness Reflecting upon and reporting on team effectiveness Identifying and implementing initiatives that help improve our overall effectiveness Measuring and reporting on the outcome of such initiatives, positive or negative Delivery teams must communicate the following information to the wider department (i.e. other delivery teams, stakeholders etc) Confirmation of sprint backlog (items and sprint priority) and goal following sprint planning Details of blockers and risks as and when they arise Sprint progress Changes to the sprint backlog should the plan needs to be updated Overview of sprint outcomes, including stories and total points delivered","title":"Product Delivery Team Responsibilities"},{"location":"3.-Sprints-%26-Teams/Daily-Scrum/","text":"Ahh! This one hasn't been written yet! Maybe you could do it?","title":"Daily Scrum"},{"location":"3.-Sprints-%26-Teams/Planning/","text":"Guide Objective: Determine a Sprint Goal and Sprint Backlog, and a plan for achieving it Organiser/Owner: Delivery Team Key stakeholders (required attendees): Delivery Team Scheduled: Start of the first day of the sprint Frequency: 1 per sprint Team split: 1 per delivery team Overview \u00b6 The team will review the candidates, their capacity for the sprint, then determine what work is required to complete the each of the candidates. They will estimate in hours each of the discrete work items that need to be performed. They will work in priority order, and stop planning when capacity is nearly matched by planned work. If the next priority item is deemed too large, the team may seek a smaller, lower priority candidate that can fit into the sprint. Agenda \u00b6 Review capacity - planned absences, known availability etc. Review candidates at a high level Speculate Sprint Goal Start planning each item, determine approximate order of tasks When capacity is filled and Sprint Backlog is finalised, discuss and align on plan for executing work Finalise Sprint Goal Communicate goal and backlog to Enablement team Tips for Success \u00b6 Be realistic about hours estimates. Optimism is likely to cause failure. Review past tasks to determine and improve estimation accuracy. Everyone in the team needs to leave with a clear understanding of the plan. DO NOT ASSUME that because someone has explained something that everyone has understood it. Speak out if you don't understand, you are equally responsible for ensuring that you leave the meeting with a good understanding of what needs to be done. If understanding of a problem is limited, plan right from the start to work in a way that improves team understanding, such as pair programming.","title":"Sprint Planning"},{"location":"3.-Sprints-%26-Teams/Planning/#overview","text":"The team will review the candidates, their capacity for the sprint, then determine what work is required to complete the each of the candidates. They will estimate in hours each of the discrete work items that need to be performed. They will work in priority order, and stop planning when capacity is nearly matched by planned work. If the next priority item is deemed too large, the team may seek a smaller, lower priority candidate that can fit into the sprint.","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Planning/#agenda","text":"Review capacity - planned absences, known availability etc. Review candidates at a high level Speculate Sprint Goal Start planning each item, determine approximate order of tasks When capacity is filled and Sprint Backlog is finalised, discuss and align on plan for executing work Finalise Sprint Goal Communicate goal and backlog to Enablement team","title":"Agenda"},{"location":"3.-Sprints-%26-Teams/Planning/#tips-for-success","text":"Be realistic about hours estimates. Optimism is likely to cause failure. Review past tasks to determine and improve estimation accuracy. Everyone in the team needs to leave with a clear understanding of the plan. DO NOT ASSUME that because someone has explained something that everyone has understood it. Speak out if you don't understand, you are equally responsible for ensuring that you leave the meeting with a good understanding of what needs to be done. If understanding of a problem is limited, plan right from the start to work in a way that improves team understanding, such as pair programming.","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Retrospective/","text":"Guide Objective: Inspect team effectiveness and identify areas for improvement Organiser/Owner: Delivery Team Key stakeholders (required attendees): All delivery team members Scheduled: Occurs the last day of the sprint Frequency: 1 per sprint Team split: 1 per delivery team, offset if possible Overview \u00b6 Each delivery team should inspect their working practices to understand ways in which they can be adapted to help improve overall effectiveness (i.e. quality, speed, sustainability). Scrum is founded upon empirical practices and outlines three pillars of empiricism: Transparency, Inspection, and Adaptation. https://scrumguides.org/scrum-guide.html#scrum-theory The team should aim to 1. use facts (i.e. data) 1. in order to draw insights (i.e. ideas about causes) 1. and then create experiments (i.e. actions that may resolve/improve effectiveness in the specific area). Suggested Agenda \u00b6 Review sprint effectiveness data. Ideally this will be the start. Review existing experiments, identify experiments that are due for review, and ensure that ongoing experiments are being performed/measured well. Explore new areas for improvement to produce new experiments. Use games and workshops to derive ideas and insights from all team members. Creating and Managing Experiments \u00b6 Creating \u00b6 Experiments must be measurable, and the team must plan how and when measurements will be taken and reviewed, to understand if the experiment was successful. Capture the experiments you are performing, for example in a Excel spreadsheet or Miro board. Capture: - Reason - i.e. because of [perceived problem (including supporting data)] - Insight - i.e. we believe this is because of [cause] - Hypotheses - i.e. we believe this can be solved/improved if we do [potential solution, plan of action] - Measures - i.e. we will monitor/record [measurable outcomes] at [specific intervals/times] - Review point - i.e. we will review success [date/after _n sprints]_ - Actions - i.e. things that must therefore be done by specific people/groups Note: - Be careful not to have too many experiments running at the same time. It may become hard to keep on top of them all - Be careful that your experiments don't contradict, negatively impact, or skew the measurements of other experiments Reviewing \u00b6 When you come to review the success of an experiment, you may decide to either: 1. Extend it, because it isn't clear what the result is yet 2. Stop it, because it was successful , and continue to perform this new behaviour 3. Stop it, because it was unsuccessful , and consider reviewing the insights drawn and then creating a new experiment 4. Pause it, because something is affecting your ability to perform the experiment well Remember to : - record the outcome, including new review dates etc. - update documentation when you establish new behaviours. Tips for Success \u00b6 Consider what data you might need to inspect and work to ensure that that data is available to you. The enablement team can help to ensure this is done. Attend the Retrospectives of the other team(s) from time to time, to learn about other approaches and what experiments are being conducted elsewhere. Invite others (from the other team or the enablement team) to facilitate your retro occasionally. It can help to ensure all team members have an equal voice if an outsider plans and runs it. Mix it up now and then! Try different techniques, games, etc. as you explore your effectiveness and look for areas to improve. As with all meetings, make sure that virtual/remote participants voices are heard. As a remote participant you are equally responsible for being heard and participating actively.","title":"Sprint Retrospective"},{"location":"3.-Sprints-%26-Teams/Retrospective/#overview","text":"Each delivery team should inspect their working practices to understand ways in which they can be adapted to help improve overall effectiveness (i.e. quality, speed, sustainability). Scrum is founded upon empirical practices and outlines three pillars of empiricism: Transparency, Inspection, and Adaptation. https://scrumguides.org/scrum-guide.html#scrum-theory The team should aim to 1. use facts (i.e. data) 1. in order to draw insights (i.e. ideas about causes) 1. and then create experiments (i.e. actions that may resolve/improve effectiveness in the specific area).","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Retrospective/#suggested-agenda","text":"Review sprint effectiveness data. Ideally this will be the start. Review existing experiments, identify experiments that are due for review, and ensure that ongoing experiments are being performed/measured well. Explore new areas for improvement to produce new experiments. Use games and workshops to derive ideas and insights from all team members.","title":"Suggested Agenda"},{"location":"3.-Sprints-%26-Teams/Retrospective/#creating-and-managing-experiments","text":"","title":"Creating and Managing Experiments"},{"location":"3.-Sprints-%26-Teams/Retrospective/#creating","text":"Experiments must be measurable, and the team must plan how and when measurements will be taken and reviewed, to understand if the experiment was successful. Capture the experiments you are performing, for example in a Excel spreadsheet or Miro board. Capture: - Reason - i.e. because of [perceived problem (including supporting data)] - Insight - i.e. we believe this is because of [cause] - Hypotheses - i.e. we believe this can be solved/improved if we do [potential solution, plan of action] - Measures - i.e. we will monitor/record [measurable outcomes] at [specific intervals/times] - Review point - i.e. we will review success [date/after _n sprints]_ - Actions - i.e. things that must therefore be done by specific people/groups Note: - Be careful not to have too many experiments running at the same time. It may become hard to keep on top of them all - Be careful that your experiments don't contradict, negatively impact, or skew the measurements of other experiments","title":"Creating"},{"location":"3.-Sprints-%26-Teams/Retrospective/#reviewing","text":"When you come to review the success of an experiment, you may decide to either: 1. Extend it, because it isn't clear what the result is yet 2. Stop it, because it was successful , and continue to perform this new behaviour 3. Stop it, because it was unsuccessful , and consider reviewing the insights drawn and then creating a new experiment 4. Pause it, because something is affecting your ability to perform the experiment well Remember to : - record the outcome, including new review dates etc. - update documentation when you establish new behaviours.","title":"Reviewing"},{"location":"3.-Sprints-%26-Teams/Retrospective/#tips-for-success","text":"Consider what data you might need to inspect and work to ensure that that data is available to you. The enablement team can help to ensure this is done. Attend the Retrospectives of the other team(s) from time to time, to learn about other approaches and what experiments are being conducted elsewhere. Invite others (from the other team or the enablement team) to facilitate your retro occasionally. It can help to ensure all team members have an equal voice if an outsider plans and runs it. Mix it up now and then! Try different techniques, games, etc. as you explore your effectiveness and look for areas to improve. As with all meetings, make sure that virtual/remote participants voices are heard. As a remote participant you are equally responsible for being heard and participating actively.","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Review/","text":"Guide Objective: Present the outcome of our sprint to business stakeholders, to gain feedback and insight Organiser/Owner: Delivery Teams Key stakeholders (required attendees): Members of the business who would like to learn about what the sprint teams have been working on, particularly those with a stake in a current piece of work Scheduled: Occurs the last day of the sprint Frequency: 1 per sprint Team split: Shared Overview \u00b6 A review of work carried out by Platform Development in the latest 2-week sprint, including demos of bug fixes and new functionality ahead of release. The review gives all interested parties an opportunity to inspect what has been built to date and time to ask questions, make observations or suggestions, and have discussions about how to best move forward. The review helps ensure we're building successful product and gives us the opportunity to get feedback from the business stakeholders who are typically not available on a daily basis. Preparation Guidelines \u00b6 Ahead of the Review meeting: - Complete the Sprint Review Template -- Planned Work = PBI / Type of Work (e.g. Feature, Bug, Tech Improvement) / Points / Status / Automated Tests Written / Release (Name & Date where known) / Demo'd by -- Unplanned Work = Unplanned Backlog Items / Points / Status / Automated Tests Written / Release (Name & Date where known) / Demo'd by -- Support Summary = Overview of support tasks undertaken - Agree a member of the Platform Development team to facilitate the meeting - Agree a member of each Delivery Team to talk through the sprint items - Discuss whether a demo is appropriate and if so, plan who will be responsible for this Agenda \u00b6 Overview of Sprint \u00b6 Sprint Name Sprint Dates Sprint Goal Points Completed Summary of Outcomes \u00b6 Planned Work -- Product Backlog Items associated with the sprint goal and who will benefit from this work -- Explanation where the sprint results do not match the sprint goal Unplanned Work -- Explanation of Unplanned Backlog Items addressed during sprint Product Demos \u00b6 Presentation of what was produced in the latest sprint Support Summary \u00b6 Brief summary of support tasks undertaken in the latest sprint Discussion \u00b6 Allow stakeholders to ask questions, understand the current state of the product and help guide its direction Feedback \u00b6 Team hear first hand if the stakeholders like what they see, if there are any changes they want and if an important feature has been missed Tips for Success \u00b6 All teams must present at every review to describe what has been accomplished and answer any questions Every team member should have the opportunity to present rather than the same person every time","title":"Sprint Review"},{"location":"3.-Sprints-%26-Teams/Review/#overview","text":"A review of work carried out by Platform Development in the latest 2-week sprint, including demos of bug fixes and new functionality ahead of release. The review gives all interested parties an opportunity to inspect what has been built to date and time to ask questions, make observations or suggestions, and have discussions about how to best move forward. The review helps ensure we're building successful product and gives us the opportunity to get feedback from the business stakeholders who are typically not available on a daily basis.","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Review/#preparation-guidelines","text":"Ahead of the Review meeting: - Complete the Sprint Review Template -- Planned Work = PBI / Type of Work (e.g. Feature, Bug, Tech Improvement) / Points / Status / Automated Tests Written / Release (Name & Date where known) / Demo'd by -- Unplanned Work = Unplanned Backlog Items / Points / Status / Automated Tests Written / Release (Name & Date where known) / Demo'd by -- Support Summary = Overview of support tasks undertaken - Agree a member of the Platform Development team to facilitate the meeting - Agree a member of each Delivery Team to talk through the sprint items - Discuss whether a demo is appropriate and if so, plan who will be responsible for this","title":"Preparation Guidelines"},{"location":"3.-Sprints-%26-Teams/Review/#agenda","text":"","title":"Agenda"},{"location":"3.-Sprints-%26-Teams/Review/#overview-of-sprint","text":"Sprint Name Sprint Dates Sprint Goal Points Completed","title":"Overview of Sprint"},{"location":"3.-Sprints-%26-Teams/Review/#summary-of-outcomes","text":"Planned Work -- Product Backlog Items associated with the sprint goal and who will benefit from this work -- Explanation where the sprint results do not match the sprint goal Unplanned Work -- Explanation of Unplanned Backlog Items addressed during sprint","title":"Summary of Outcomes"},{"location":"3.-Sprints-%26-Teams/Review/#product-demos","text":"Presentation of what was produced in the latest sprint","title":"Product Demos"},{"location":"3.-Sprints-%26-Teams/Review/#support-summary","text":"Brief summary of support tasks undertaken in the latest sprint","title":"Support Summary"},{"location":"3.-Sprints-%26-Teams/Review/#discussion","text":"Allow stakeholders to ask questions, understand the current state of the product and help guide its direction","title":"Discussion"},{"location":"3.-Sprints-%26-Teams/Review/#feedback","text":"Team hear first hand if the stakeholders like what they see, if there are any changes they want and if an important feature has been missed","title":"Feedback"},{"location":"3.-Sprints-%26-Teams/Review/#tips-for-success","text":"All teams must present at every review to describe what has been accomplished and answer any questions Every team member should have the opportunity to present rather than the same person every time","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/","text":"Guide Objective: Map out the deliverables for next several sprints Organiser/Owner: Product Delivery Teams Key stakeholders (required attendees): Anyone who wants something delivered in upcoming sprints Scheduled: Occurs the day before the sprint starts, or earlier. Frequency: Fortnightly Team split: 1 per delivery team, offset as enablement team required Overview \u00b6 Teams must look ahead and map out how the current highest priority backlog items will be delivered over the coming sprints (e.g. next 3-6 sprints). Each product delivery team has their own sprint runway plan. This will be a dynamic plan, not a fix plan. Doing so enables the team to understand the impact of pushing work out of the current sprint - i.e. if we can't do this now, when will we, and what else will be impacted. This supports the team's autonomy, and empowers them to make informed decisions about the best way to complete work. Inputs \u00b6 (e.g. things that have happened before the session) Stakeholders - which includes all members of the department - should add items to the backlog, they may speculatively add items into future sprints as well. Backlog items will be varying degrees of ready. The more ready it is, the more likely it can be addressed soon! The existing sprint runway plan forms the starting point. Each session refines and extends this plan. Activities & Outcomes \u00b6 If a stakeholder has added something to a sprint, they should use this session to explain why it is important to be done at this time. Together the team can discuss the impact of this item: what needs to be moved to accommodate it, or should this be pushed back/moved forwards. The team can add more items into sprints, from the backlog. Anyone is welcome to make suggestions. By the end of the session the team should be aligned on what is likely to be in the upcoming sprints - this is their \"sprint runway plan\". It should cover 3 - 6 sprints. The team should have a high level of confidence in the achievability of the very next sprint. Further ahead each sprints will gradually be less certain. Relative Sprint Priority \u00b6 Priority of backlog items should be ranked using standard criteria, such as impact and urgency, and should not be subjective. Relative Sprint Priority ranks sprint deliverables, i.e. backlog items within the context of a sprint. For example, multiple backlog items may have P2 or P3 priority against them, so they can be ordered in a way that indicated the priority within the context of this specific sprint. Equally, given wider implications (such as blockers or dependencies) it may be preferable or necessary to work on a lower priority item before a higher one. The product delivery team are responsible for determining their own Relative Sprint Priority. Stakeholders may of course offer their feedback. Tips for Success \u00b6 Remember that you are planning to be dynamic, not creating a rigid plan. Even though you are planning speculatively, you should try to be realistic. Don't overfill upcoming sprints. It will be tempting when stakeholders are all wanting competing work to be complete, but the team understand their competency better than anyone else, so they must communicate what they feel is achievable, and not just what they think stakeholders want to hear. Good estimation will help with good planning. It is important that we measure the success and impact of our estimates. The closer to \"Ready\" upcoming work is, the easier it will be to plan for it. Make sure you set aside time to get upcoming work ready .","title":"Runway Planning"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#overview","text":"Teams must look ahead and map out how the current highest priority backlog items will be delivered over the coming sprints (e.g. next 3-6 sprints). Each product delivery team has their own sprint runway plan. This will be a dynamic plan, not a fix plan. Doing so enables the team to understand the impact of pushing work out of the current sprint - i.e. if we can't do this now, when will we, and what else will be impacted. This supports the team's autonomy, and empowers them to make informed decisions about the best way to complete work.","title":"Overview"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#inputs","text":"(e.g. things that have happened before the session) Stakeholders - which includes all members of the department - should add items to the backlog, they may speculatively add items into future sprints as well. Backlog items will be varying degrees of ready. The more ready it is, the more likely it can be addressed soon! The existing sprint runway plan forms the starting point. Each session refines and extends this plan.","title":"Inputs"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#activities-outcomes","text":"If a stakeholder has added something to a sprint, they should use this session to explain why it is important to be done at this time. Together the team can discuss the impact of this item: what needs to be moved to accommodate it, or should this be pushed back/moved forwards. The team can add more items into sprints, from the backlog. Anyone is welcome to make suggestions. By the end of the session the team should be aligned on what is likely to be in the upcoming sprints - this is their \"sprint runway plan\". It should cover 3 - 6 sprints. The team should have a high level of confidence in the achievability of the very next sprint. Further ahead each sprints will gradually be less certain.","title":"Activities &amp; Outcomes"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#relative-sprint-priority","text":"Priority of backlog items should be ranked using standard criteria, such as impact and urgency, and should not be subjective. Relative Sprint Priority ranks sprint deliverables, i.e. backlog items within the context of a sprint. For example, multiple backlog items may have P2 or P3 priority against them, so they can be ordered in a way that indicated the priority within the context of this specific sprint. Equally, given wider implications (such as blockers or dependencies) it may be preferable or necessary to work on a lower priority item before a higher one. The product delivery team are responsible for determining their own Relative Sprint Priority. Stakeholders may of course offer their feedback.","title":"Relative Sprint Priority"},{"location":"3.-Sprints-%26-Teams/Runway-Planning/#tips-for-success","text":"Remember that you are planning to be dynamic, not creating a rigid plan. Even though you are planning speculatively, you should try to be realistic. Don't overfill upcoming sprints. It will be tempting when stakeholders are all wanting competing work to be complete, but the team understand their competency better than anyone else, so they must communicate what they feel is achievable, and not just what they think stakeholders want to hear. Good estimation will help with good planning. It is important that we measure the success and impact of our estimates. The closer to \"Ready\" upcoming work is, the easier it will be to plan for it. Make sure you set aside time to get upcoming work ready .","title":"Tips for Success"},{"location":"3.-Sprints-%26-Teams/Team-Structure/","text":"Team Structure \u00b6 The Platform Development department is split up into a number of self-organised, cross-functional Product Delivery Teams , plus an Enablement Team. The team structure as of Feb 2022 is as follows: Anzu \u00b6 Team Anzu (aka the 'Internal' Delivery Team) is focused on the needs of Adam Matthew publications; delivering value to traditional AM product lines in Quartex, as well as supporting White Label and AM Explorer. Yoshi \u00b6 Team Yoshi are the 'External' Delivery Team. Their work is based around needs of external Quartex Customers, delivering value to Sales, Customer Experience and the external organisations that are using the Quartex platform. New Delivery Team! \u00b6 We are currently recruiting for a third Delivery Team to who will concentrate on delivering reporting, analytics and other tools to meet the needs of Adam Matthew. Archaeopteryx (ArchOps) \u00b6 This 'Tech Facilitation' team, is a specialised Delivery Team. It provides improvements to infrustructure and architcecutre, as well as investigating new approaches & technology that could be used by the Delivery Teams. In other words, it delivers value to the other Delivery Teams! It is made up of our Software Architect and DevOps Engineer. Enablement Team \u00b6 The Enablement Team is here to facilitate, support and otherwise enable the Delivery Teams to carry out their work. If a team is wrestling with a particular challenge, there may be a member of the Enablement Team who can help them. It consists of the VP Engineering, Senior Product Manager, Tech Support Manager and UX Designer.","title":"Team Structure"},{"location":"3.-Sprints-%26-Teams/Team-Structure/#team-structure","text":"The Platform Development department is split up into a number of self-organised, cross-functional Product Delivery Teams , plus an Enablement Team. The team structure as of Feb 2022 is as follows:","title":"Team Structure"},{"location":"3.-Sprints-%26-Teams/Team-Structure/#anzu","text":"Team Anzu (aka the 'Internal' Delivery Team) is focused on the needs of Adam Matthew publications; delivering value to traditional AM product lines in Quartex, as well as supporting White Label and AM Explorer.","title":"Anzu"},{"location":"3.-Sprints-%26-Teams/Team-Structure/#yoshi","text":"Team Yoshi are the 'External' Delivery Team. Their work is based around needs of external Quartex Customers, delivering value to Sales, Customer Experience and the external organisations that are using the Quartex platform.","title":"Yoshi"},{"location":"3.-Sprints-%26-Teams/Team-Structure/#new-delivery-team","text":"We are currently recruiting for a third Delivery Team to who will concentrate on delivering reporting, analytics and other tools to meet the needs of Adam Matthew.","title":"New Delivery Team!"},{"location":"3.-Sprints-%26-Teams/Team-Structure/#archaeopteryx-archops","text":"This 'Tech Facilitation' team, is a specialised Delivery Team. It provides improvements to infrustructure and architcecutre, as well as investigating new approaches & technology that could be used by the Delivery Teams. In other words, it delivers value to the other Delivery Teams! It is made up of our Software Architect and DevOps Engineer.","title":"Archaeopteryx (ArchOps)"},{"location":"3.-Sprints-%26-Teams/Team-Structure/#enablement-team","text":"The Enablement Team is here to facilitate, support and otherwise enable the Delivery Teams to carry out their work. If a team is wrestling with a particular challenge, there may be a member of the Enablement Team who can help them. It consists of the VP Engineering, Senior Product Manager, Tech Support Manager and UX Designer.","title":"Enablement Team"},{"location":"4.-Backlog-Management/","text":"Who is this for? \u00b6 This section describes the way in which we turn roadmap items into backlog items which are ready to be built. It is intended for anyone engaged in creating ready backlog items.","title":"Backlog Management"},{"location":"4.-Backlog-Management/#who-is-this-for","text":"This section describes the way in which we turn roadmap items into backlog items which are ready to be built. It is intended for anyone engaged in creating ready backlog items.","title":"Who is this for?"},{"location":"4.-Backlog-Management/Backlog-Item-Types/","text":"Overview \u00b6 A backlog is a list of discrete things we have determined we want to deliver, in order progress out roadmap. A roadmap item therefore has one or more backlog items that must be delivered. Backlog items deliver different types of things, and therefore take different forms. Understanding and distinguishing the type of backlog item helps us determine how to prepare it , estimate it, and plan for it. Once backlog items are ready, they can be prioritised within the Product Backlog so that they can be selected as candidates for upcoming sprints Type of Deliverable Purpose Process Features and Changes Deliver Product Roadmap items Defining Features and Changes Technical improvements Deliver Technical Roadmap items Defining Technical Improvements Bugs Progressed to 3rd Line Support from 2nd Line Support TBC Knowledge Acquisition Support our ability to define and ready the above types TBC","title":"Backlog Item Types"},{"location":"4.-Backlog-Management/Backlog-Item-Types/#overview","text":"A backlog is a list of discrete things we have determined we want to deliver, in order progress out roadmap. A roadmap item therefore has one or more backlog items that must be delivered. Backlog items deliver different types of things, and therefore take different forms. Understanding and distinguishing the type of backlog item helps us determine how to prepare it , estimate it, and plan for it. Once backlog items are ready, they can be prioritised within the Product Backlog so that they can be selected as candidates for upcoming sprints Type of Deliverable Purpose Process Features and Changes Deliver Product Roadmap items Defining Features and Changes Technical improvements Deliver Technical Roadmap items Defining Technical Improvements Bugs Progressed to 3rd Line Support from 2nd Line Support TBC Knowledge Acquisition Support our ability to define and ready the above types TBC","title":"Overview"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/","text":"Who is this for? \u00b6 Describes the way in which we turn roadmap items into ready backlog items. This document is indented for anyone engaged in creating ready backlog items. Quienes son los 3 Amigos? \u00b6 The term \"3 Amigos\" refers to the perspectives that are generally required to effectively define the requirements of a roadmap item, be it product, technical, or support: Perspective Identifier Business Perspective in the form of the \"Product Owner\", usually performed by a Product Manager for product roadmap items, support manager for support items etc, tech lead for tech items Engineering Perspective in the form of a \"Technical Lead\" (usually a software engineer or architect) Quality Assurance Perspective in the form of a \"Test Lead\" (usually a test engineer) As such each Roadmap item is assigned a Product Owner, a Technical Lead, and a Test Lead. Although there are typical business roles that map to the 3A role (indicated above), each of them can be assigned to any person who can perform the duties effectively. The three amigos approach aims to ensure that the smallest group of people possible will collaborate to create a ready backlog item. It is of course possible therefore that more than 3 people will be required, for example, a Design lead where there are UI components to deliver. The three Amigos will work with other stakeholders from across the business as they go through Definition phase. For example, the Tech Lead should consult with other engineers and the Architect. `// TODO: Who should determine the 3 Amigos? How and When is this done? Responsibilities \u00b6 The Three Amigos are collectively responsible for: - creating ready backlog items that describe an effective solution to the problem - ensuring that ready backlog items are produced \"in time\", and communicating with the Head of Product about their progress and blockers. - ensuring that they engage with the right stakeholders - recording their activities and decisions - planning and executing the definition work - supporting the build and release phase by resolving queries and updating requirements `// TODO: Set out the specific responsibilities of each of the 3A roles. Backlog Prioritisation \u00b6 `// TODO: describe impact & urgency ranking Sprint Candidates \u00b6 `// TODO: describe impact & urgency ranking","title":"3 Amigos & Readying Backlog Items"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#who-is-this-for","text":"Describes the way in which we turn roadmap items into ready backlog items. This document is indented for anyone engaged in creating ready backlog items.","title":"Who is this for?"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#quienes-son-los-3-amigos","text":"The term \"3 Amigos\" refers to the perspectives that are generally required to effectively define the requirements of a roadmap item, be it product, technical, or support: Perspective Identifier Business Perspective in the form of the \"Product Owner\", usually performed by a Product Manager for product roadmap items, support manager for support items etc, tech lead for tech items Engineering Perspective in the form of a \"Technical Lead\" (usually a software engineer or architect) Quality Assurance Perspective in the form of a \"Test Lead\" (usually a test engineer) As such each Roadmap item is assigned a Product Owner, a Technical Lead, and a Test Lead. Although there are typical business roles that map to the 3A role (indicated above), each of them can be assigned to any person who can perform the duties effectively. The three amigos approach aims to ensure that the smallest group of people possible will collaborate to create a ready backlog item. It is of course possible therefore that more than 3 people will be required, for example, a Design lead where there are UI components to deliver. The three Amigos will work with other stakeholders from across the business as they go through Definition phase. For example, the Tech Lead should consult with other engineers and the Architect. `// TODO: Who should determine the 3 Amigos? How and When is this done?","title":"Quienes son los 3 Amigos?"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#responsibilities","text":"The Three Amigos are collectively responsible for: - creating ready backlog items that describe an effective solution to the problem - ensuring that ready backlog items are produced \"in time\", and communicating with the Head of Product about their progress and blockers. - ensuring that they engage with the right stakeholders - recording their activities and decisions - planning and executing the definition work - supporting the build and release phase by resolving queries and updating requirements `// TODO: Set out the specific responsibilities of each of the 3A roles.","title":"Responsibilities"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#backlog-prioritisation","text":"`// TODO: describe impact & urgency ranking","title":"Backlog Prioritisation"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/#sprint-candidates","text":"`// TODO: describe impact & urgency ranking","title":"Sprint Candidates"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Accessibility-Impact-Assessment/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Accessibility Impact Assessment"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Architectural-Design/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Architectural Design"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Functional-Requirements-with-BDD/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Functional Requirements with BDD"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Non-Functional-Requirements/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Non Functional Requirements"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Privacy-Impact-Assessments/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Privacy Impact Assessments"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Problem%2C-Value%2C-Solution-Statements/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Problem, Value, Solution Statements"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Risk%2C-Assumptions%2C-Issues%2C-%26-Dependencies/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Risk, Assumptions, Issues, & Dependencies"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/Security-Impact-Assessment/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"Security Impact Assessment"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/UI-%26-UX-Designs/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"UI & UX Designs"},{"location":"4.-Backlog-Management/3-Amigos-%26-Readying-Backlog-Items/User-Flow-Diagrams/","text":"This document hasn't been written yet! Perhaps you could do it?","title":"User Flow Diagrams"},{"location":"4.-Backlog-Management/Definition-of-Ready/","text":"What's a Definition of Ready? \u00b6 A ready backlog item is one that can be immediately actioned (or worked on ) by a delivery team. That means it must have enough information in it in order for it to be delivered by the team fast and effectively. The Definition of Ready (DOR), therefore, outlines the characteristics and information - described as artefacts - that a delivery team would need in order to action an backlog item. However, caution must be taken to ensure that the DOR does not become a set of linear gates that effectively prevent us working in an agile way. We must balance the benefits of having a clear and aligned understanding of our objective that enables with being able to adapt quickly. Each backlog item type will have it's own DOR, and corresponding artefacts. 2 Key DORs are: Features and Changes Technical Improvements Information regarding producing each artefact type can be found within the 3 Amigos & Readying Backlog Items section. Note: The Definition of Ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes. Backlog Prioritisation \u00b6 `// TODO: describe impact & urgency ranking Sprint Candidates \u00b6 `// TODO: describe impact & urgency ranking","title":"Definition of Ready"},{"location":"4.-Backlog-Management/Definition-of-Ready/#whats-a-definition-of-ready","text":"A ready backlog item is one that can be immediately actioned (or worked on ) by a delivery team. That means it must have enough information in it in order for it to be delivered by the team fast and effectively. The Definition of Ready (DOR), therefore, outlines the characteristics and information - described as artefacts - that a delivery team would need in order to action an backlog item. However, caution must be taken to ensure that the DOR does not become a set of linear gates that effectively prevent us working in an agile way. We must balance the benefits of having a clear and aligned understanding of our objective that enables with being able to adapt quickly. Each backlog item type will have it's own DOR, and corresponding artefacts. 2 Key DORs are: Features and Changes Technical Improvements Information regarding producing each artefact type can be found within the 3 Amigos & Readying Backlog Items section. Note: The Definition of Ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes.","title":"What's a Definition of Ready?"},{"location":"4.-Backlog-Management/Definition-of-Ready/#backlog-prioritisation","text":"`// TODO: describe impact & urgency ranking","title":"Backlog Prioritisation"},{"location":"4.-Backlog-Management/Definition-of-Ready/#sprint-candidates","text":"`// TODO: describe impact & urgency ranking","title":"Sprint Candidates"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/","text":"Definition of Ready \u00b6 Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary User Flow diagrams UI & UX designs Functional Requirements , expressed as: BDD Scenarios BDD Business Rules Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log) Definition Process Flow \u00b6 `// TODO: Determine diagramming needs. Is Mermaid sufficient? Should we link out to Miro, import images, or use something else? The process flow details an example of the way a roadmap item is understood and eventually turned into well defined backlog items. Note that the specific mechanisms are generally purposefully excluded, such that different techniques and tools can be used as desired by the team. What matters is that we are able to define great solutions that read for delivery teams as fast and effectively as possible, not how that comes about. Planning and Scoping \u00b6 \"Scoping\" of the roadmap item includes the solicitation of requirements from stakeholders and users, and the definition what the scope of deliverable. Generally, this includes who (which users) will be able to do what , and why . This would be accompanied by the specific needs of the users, as well as any specific exclusions (i.e. things that are out of scope). This information will inform the functional and non-functional requirements of resulting backlog items. Once the scope is known, the 3As can plan the specific activities they wish to perform as through the Definition phase - including how long this will take and when. Technical Scoping \u00b6 This stage include understanding technical scope. This is not to be confused with technical design, which comes later. At this point we know what problem we are trying to solve, and we are looking to understand the technology constraints. This could include: - 3rd party technologies specifically required - for example is there are pre-defined 3rd party API we must integrate with, and what are its limitations and benefits. - Other work that may block or impact. - Existing platform limitations or competencies. Initial Design \u00b6 The initial design stage covers researching and then creating one or more design options. Design does not necessarily mean creating visual designs. Again there are no fixed practices for this stage. Each problem will require its own approach and it is up to the 3As to agree on the approach taken. This is a great time to think about what additional knowledge could be needed. It's quite possible that a desirable solution raises a number of questions about technical feasibility. Do we need to embark on an investigation - such as creating a proof of concept, understanding some technical documentation, etc? Define high-level solution \u00b6 Armed with the scope and some potential solutions, a high level can be defined. A high level solution to a roadmap item should be summarised and, where appropriate, supported with UI designs and a technical overview. Low Res Design & User Flow \u00b6 \"Low Res\" refers to a low degree of detail, and example of this is wireframing. Coupled with user flow diagrams, these provide the backbone to our functional requirements. Create Backlog Items \u00b6 The 3As must split the roadmap item into backlog items. Initially this is done by creating \"shell\" backlog items with the basic information of what it will deliver, the following steps will progress from here to make them \"ready\". Turning large solutions into discrete deliverables is an art form! Our definition of ready provides some guidelines of what a backlog item should be, generally centred around the INVEST principles. Further reading and learning on this topic is highly recommended. High Res designs \u00b6 `// TODO: complete high res designs section Functional and Non-Functional Requirements \u00b6 `// TODO: complete Reqs section Technical approach \u00b6 `// TODO: complete tech approach section Team Review \u00b6 `// TODO: complete review section","title":"Features and Changes"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#definition-of-ready","text":"Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary User Flow diagrams UI & UX designs Functional Requirements , expressed as: BDD Scenarios BDD Business Rules Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log)","title":"Definition of Ready"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#definition-process-flow","text":"`// TODO: Determine diagramming needs. Is Mermaid sufficient? Should we link out to Miro, import images, or use something else? The process flow details an example of the way a roadmap item is understood and eventually turned into well defined backlog items. Note that the specific mechanisms are generally purposefully excluded, such that different techniques and tools can be used as desired by the team. What matters is that we are able to define great solutions that read for delivery teams as fast and effectively as possible, not how that comes about.","title":"Definition Process Flow"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#planning-and-scoping","text":"\"Scoping\" of the roadmap item includes the solicitation of requirements from stakeholders and users, and the definition what the scope of deliverable. Generally, this includes who (which users) will be able to do what , and why . This would be accompanied by the specific needs of the users, as well as any specific exclusions (i.e. things that are out of scope). This information will inform the functional and non-functional requirements of resulting backlog items. Once the scope is known, the 3As can plan the specific activities they wish to perform as through the Definition phase - including how long this will take and when.","title":"Planning and Scoping"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#technical-scoping","text":"This stage include understanding technical scope. This is not to be confused with technical design, which comes later. At this point we know what problem we are trying to solve, and we are looking to understand the technology constraints. This could include: - 3rd party technologies specifically required - for example is there are pre-defined 3rd party API we must integrate with, and what are its limitations and benefits. - Other work that may block or impact. - Existing platform limitations or competencies.","title":"Technical Scoping"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#initial-design","text":"The initial design stage covers researching and then creating one or more design options. Design does not necessarily mean creating visual designs. Again there are no fixed practices for this stage. Each problem will require its own approach and it is up to the 3As to agree on the approach taken. This is a great time to think about what additional knowledge could be needed. It's quite possible that a desirable solution raises a number of questions about technical feasibility. Do we need to embark on an investigation - such as creating a proof of concept, understanding some technical documentation, etc?","title":"Initial Design"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#define-high-level-solution","text":"Armed with the scope and some potential solutions, a high level can be defined. A high level solution to a roadmap item should be summarised and, where appropriate, supported with UI designs and a technical overview.","title":"Define high-level solution"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#low-res-design-user-flow","text":"\"Low Res\" refers to a low degree of detail, and example of this is wireframing. Coupled with user flow diagrams, these provide the backbone to our functional requirements.","title":"Low Res Design &amp; User Flow"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#create-backlog-items","text":"The 3As must split the roadmap item into backlog items. Initially this is done by creating \"shell\" backlog items with the basic information of what it will deliver, the following steps will progress from here to make them \"ready\". Turning large solutions into discrete deliverables is an art form! Our definition of ready provides some guidelines of what a backlog item should be, generally centred around the INVEST principles. Further reading and learning on this topic is highly recommended.","title":"Create Backlog Items"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#high-res-designs","text":"`// TODO: complete high res designs section","title":"High Res designs"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#functional-and-non-functional-requirements","text":"`// TODO: complete Reqs section","title":"Functional and Non-Functional Requirements"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#technical-approach","text":"`// TODO: complete tech approach section","title":"Technical approach"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Features-and-Changes/#team-review","text":"`// TODO: complete review section","title":"Team Review"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Technical-Improvements/","text":"Definition of Ready \u00b6 Note: The definition of ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes. Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary Measures for success and mechanisms for producing metrics Technical requirements, such as: Behaviours needed from tools and teams Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log)","title":"Technical Improvements"},{"location":"4.-Backlog-Management/Definition-of-Ready/Defining-Technical-Improvements/#definition-of-ready","text":"Note: The definition of ready should be considered a guide rather than a set of hard rules. The artefacts actually needed will vary depending on the nature of the backlog item. It is up to the 3 Amigos team to determine what readiness means for each backlog item as they go through the definition processes. Ready Features and Change Backlog Items should: Meet INVEST criteria and Include the following artefacts: Problem Statement Value Proposition Solution Executive Summary Measures for success and mechanisms for producing metrics Technical requirements, such as: Behaviours needed from tools and teams Non-functional requirements including: Non-functional acceptance criteria (above and beyond platform-wide standards) Privacy Impact Assessment * Security Impact Assessment * Accessibility Impact Assessment * Architectural design Risks, Assumptions, Issues, and Dependencies Rationalisation - any sporting information that provides context to the artefacts above (for example, a decision log)","title":"Definition of Ready"},{"location":"5.-Collaboration-%26-Ops/","text":"What & who is this for? \u00b6 We all collaborate & communicate with each other every day. This is possibly the most critical core competency effecting our journey of continuous improvement and mission success. As a largely remote & distributed team that is growing and changing shape, we must learn to master this in new ways. This section will outline some of the principles, processes and practices surrounding how we collaborate & communicate, as well as other operational tools and techniques. The aim is to ensure that we operate as effectively, reliably, and sustainably as possible. Everyone in our department must be familiar with the contents of this section. Guiding Principles \u00b6 Stop judging, start helping - If you don't like the way something is, or what someone is doing, complaining isn't going to change it. A collaborative environment requires us all to identify how we can contribute. Focus your energy on what you can do to make the situation better, rather than why the situation is bad. No side chats - When discussing work, do so in a public forum - e.g. the relevant Teams channel. Regardless of who the question or comment is for, it benefits everyone involved in the work to hear that it has been said. Over-communicate - Being bombarded by messages is bad, but not being informed of important information is much worse. And if it is worth saying once, it is worth saying again and again. Just because you have told someone something, it doesn't mean they have remembered it or understood it. Over-communicating isn't oversharing though; succinctness is kindness, critical detail should follow. Ask stupid questions - There are no stupid questions - if someone is explaining something and you're unsure about it, ask them to explain it again. If they are not explaining in a way that resonates with you, ask if they can explain it in a way that does. If someone has told you something and you don't remember what they said, be comfortable asking them again - we are all growing and you won't be able to remember everything you are told. Be transparent & informative - Making information available to others is transparency. Directly telling people that information is being informant. Good communication requires both. Offer and ask for feedback - We all need to improve. Useful feedback is a gift of immeasurable value. You'll get more of it if you go looking for it. And remember, you don't have to like it. Call it out - If someone isn't following one of these principles, let that person know. Remind each other what great collaboration and communication looks like. We all get it wrong some times.","title":"Collaboration & Ops"},{"location":"5.-Collaboration-%26-Ops/#what-who-is-this-for","text":"We all collaborate & communicate with each other every day. This is possibly the most critical core competency effecting our journey of continuous improvement and mission success. As a largely remote & distributed team that is growing and changing shape, we must learn to master this in new ways. This section will outline some of the principles, processes and practices surrounding how we collaborate & communicate, as well as other operational tools and techniques. The aim is to ensure that we operate as effectively, reliably, and sustainably as possible. Everyone in our department must be familiar with the contents of this section.","title":"What &amp; who is this for?"},{"location":"5.-Collaboration-%26-Ops/#guiding-principles","text":"Stop judging, start helping - If you don't like the way something is, or what someone is doing, complaining isn't going to change it. A collaborative environment requires us all to identify how we can contribute. Focus your energy on what you can do to make the situation better, rather than why the situation is bad. No side chats - When discussing work, do so in a public forum - e.g. the relevant Teams channel. Regardless of who the question or comment is for, it benefits everyone involved in the work to hear that it has been said. Over-communicate - Being bombarded by messages is bad, but not being informed of important information is much worse. And if it is worth saying once, it is worth saying again and again. Just because you have told someone something, it doesn't mean they have remembered it or understood it. Over-communicating isn't oversharing though; succinctness is kindness, critical detail should follow. Ask stupid questions - There are no stupid questions - if someone is explaining something and you're unsure about it, ask them to explain it again. If they are not explaining in a way that resonates with you, ask if they can explain it in a way that does. If someone has told you something and you don't remember what they said, be comfortable asking them again - we are all growing and you won't be able to remember everything you are told. Be transparent & informative - Making information available to others is transparency. Directly telling people that information is being informant. Good communication requires both. Offer and ask for feedback - We all need to improve. Useful feedback is a gift of immeasurable value. You'll get more of it if you go looking for it. And remember, you don't have to like it. Call it out - If someone isn't following one of these principles, let that person know. Remind each other what great collaboration and communication looks like. We all get it wrong some times.","title":"Guiding Principles"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/","text":"Sickness & Unplanned Leave \u00b6 Unplanned leave is any short notice leave that has not been booked in advance. It happens, and that's OK . As we, by definition, did not plan for it, our plans will probably be affected by it. Follow the HR guidelines on communicating unplanned absence. Additionally, if you are able to, send a message using Teams to key channels (e.g. the relevant Delivery Team channel, any 3As groups etc) to inform them that you are not working. This will help the team to rapidly re-plan any work as required. Engineers Tip You should always commit and push changes to your working branch at the end of the working day, including descriptive comments about the work in progress. You can never guarantee you'll be in a position to pick up where you left off the next day! Annual Leave & Planned Leave \u00b6 Warning If you are planning to miss a Daily Scrum for any reason, you MUST make sure you provide an update of your WIP to the team in advance and that the Sprint Board is updated with your progress Requesting Annual Leave \u00b6 Before requesting annual leave, check the planner for the dates you want to book off. A first come, first serve approach is applied, so booking early is advised, especially for longer periods of absence. We need to make sure that each of the delivery teams is able to continue working in your absence, so if other members of your team are already booked off, then you need to have a conversation with your team about the impact of additional impact this might have. If you determine that the team can continue effectively with the remaining resource you are welcome to submit the request. Use the comments field in YouManage to detail the outcome of the conversation. Engineers: If you are on on the rota to act as Support Engineer during the period that you wish to book off, you should contact the Support Manager and assist them in adjusting the rota (for longer periods of absence) or finding cover (for shorter periods). DO NOT LEAVE IT TO YOUR LINE MANAGER TO WORK THIS OUT FOR YOU. The earlier you book the easier it will be to plan for your absence The Longer your absence period is, the more advanced notice you should give, as a guide you should aim to give AT LEAST 2 x the booking period. (e.g. 2 days off - 4 days notice, 2 weeks off - 4 weeks notice). If there are meetings during your planned absence you must notify the meeting organiser(s). If you are integral to the meeting you should help to reorganise, if you are not then ensure that you work with the organiser to prepare and communicate any required input in advance. This includes all sprint meeting ESPECIALLY THE DAILY SCRUM. Christmas Period \u00b6 Christmas period bookings will not be accepted until later in the year, when we will have a clearer understanding of the anticipated workload and resource availability during that period. First come first serve does not apply, as this often yields unfair results. Specific details of how to submit requests will be communicated with the team when the time comes. Usually around late September. Please do not request annual leave over the Christmas and New Years period via YouManage until this communication has been made. Preparing for Annual Leave \u00b6 Generally, you must ensure that anyone you work with regularly or are likely to be working with around the time of your leave is aware of your planned absence. Specifically, ensure that each of the following are performed: When your annual leave request has been approved, Ensure that your team are aware of this as soon as possible Take proactive steps to organise and perform any re-planning necessary Put the leave in your calendar, mark it as out of office Before the sprint (s) in which your annual leave occurs: If you are going to miss sprint planning, ensure that your capacity is recorded in advance or request a team member to do so for you. Ask the team to provide an update on the sprint plan on your return At the start of the sprint (s) in which your annual leave occurs: Ensure that your capacity is adjusted during planning Ensure that the sprint work is planned around your absence, especially where work depends on yourself. The day before your leave Set your out of office in Outlook and Teams Remind your team that you will be on leave and when you will be returning. Provide the team with an update of your work in progress and provide a handover if required. Engineers: you must ensure that any WIP code is committed to your working branch and pushed to the remote, including descriptive comments about the work in progress. Support Engineer: If you are currently acting as Support Engineer when your leave starts, ensure that you provide a clear and complete handover to the Support Manager and the engineer who is covering during your leave. Returning from Annual Leave \u00b6 Work with your team to get yourself up to speed. You are responsible for organising this. You must prioritise this and ensure that you understand the current sprint plan and progress. Hybrid Working Framework \u00b6 `// TODO: link out here","title":"Absence & Leave"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#sickness-unplanned-leave","text":"Unplanned leave is any short notice leave that has not been booked in advance. It happens, and that's OK . As we, by definition, did not plan for it, our plans will probably be affected by it. Follow the HR guidelines on communicating unplanned absence. Additionally, if you are able to, send a message using Teams to key channels (e.g. the relevant Delivery Team channel, any 3As groups etc) to inform them that you are not working. This will help the team to rapidly re-plan any work as required. Engineers Tip You should always commit and push changes to your working branch at the end of the working day, including descriptive comments about the work in progress. You can never guarantee you'll be in a position to pick up where you left off the next day!","title":"Sickness &amp; Unplanned Leave"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#annual-leave-planned-leave","text":"Warning If you are planning to miss a Daily Scrum for any reason, you MUST make sure you provide an update of your WIP to the team in advance and that the Sprint Board is updated with your progress","title":"Annual Leave &amp; Planned Leave"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#requesting-annual-leave","text":"Before requesting annual leave, check the planner for the dates you want to book off. A first come, first serve approach is applied, so booking early is advised, especially for longer periods of absence. We need to make sure that each of the delivery teams is able to continue working in your absence, so if other members of your team are already booked off, then you need to have a conversation with your team about the impact of additional impact this might have. If you determine that the team can continue effectively with the remaining resource you are welcome to submit the request. Use the comments field in YouManage to detail the outcome of the conversation. Engineers: If you are on on the rota to act as Support Engineer during the period that you wish to book off, you should contact the Support Manager and assist them in adjusting the rota (for longer periods of absence) or finding cover (for shorter periods). DO NOT LEAVE IT TO YOUR LINE MANAGER TO WORK THIS OUT FOR YOU. The earlier you book the easier it will be to plan for your absence The Longer your absence period is, the more advanced notice you should give, as a guide you should aim to give AT LEAST 2 x the booking period. (e.g. 2 days off - 4 days notice, 2 weeks off - 4 weeks notice). If there are meetings during your planned absence you must notify the meeting organiser(s). If you are integral to the meeting you should help to reorganise, if you are not then ensure that you work with the organiser to prepare and communicate any required input in advance. This includes all sprint meeting ESPECIALLY THE DAILY SCRUM.","title":"Requesting Annual Leave"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#christmas-period","text":"Christmas period bookings will not be accepted until later in the year, when we will have a clearer understanding of the anticipated workload and resource availability during that period. First come first serve does not apply, as this often yields unfair results. Specific details of how to submit requests will be communicated with the team when the time comes. Usually around late September. Please do not request annual leave over the Christmas and New Years period via YouManage until this communication has been made.","title":"Christmas Period"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#preparing-for-annual-leave","text":"Generally, you must ensure that anyone you work with regularly or are likely to be working with around the time of your leave is aware of your planned absence. Specifically, ensure that each of the following are performed: When your annual leave request has been approved, Ensure that your team are aware of this as soon as possible Take proactive steps to organise and perform any re-planning necessary Put the leave in your calendar, mark it as out of office Before the sprint (s) in which your annual leave occurs: If you are going to miss sprint planning, ensure that your capacity is recorded in advance or request a team member to do so for you. Ask the team to provide an update on the sprint plan on your return At the start of the sprint (s) in which your annual leave occurs: Ensure that your capacity is adjusted during planning Ensure that the sprint work is planned around your absence, especially where work depends on yourself. The day before your leave Set your out of office in Outlook and Teams Remind your team that you will be on leave and when you will be returning. Provide the team with an update of your work in progress and provide a handover if required. Engineers: you must ensure that any WIP code is committed to your working branch and pushed to the remote, including descriptive comments about the work in progress. Support Engineer: If you are currently acting as Support Engineer when your leave starts, ensure that you provide a clear and complete handover to the Support Manager and the engineer who is covering during your leave.","title":"Preparing for Annual Leave"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#returning-from-annual-leave","text":"Work with your team to get yourself up to speed. You are responsible for organising this. You must prioritise this and ensure that you understand the current sprint plan and progress.","title":"Returning from Annual Leave"},{"location":"5.-Collaboration-%26-Ops/Absence-Leave/#hybrid-working-framework","text":"`// TODO: link out here","title":"Hybrid Working Framework"},{"location":"6.-Engineering/","text":"This section contains an overview of practices, tools, principles, and patterns used by engineers in our teams, as well as guidelines for how to follow them. This is our \"house style\". All engineers must ensure that they understand and are able to follow these guidelines. For the most part they are not rules however. Guidelines must be followed until there is a reasonable case to not do so. In this instance you should ensure the guidelines are then updated if appropriate. Note Specific implementation details should be included within the ReadMe files of the relevant repositories or subdirectories therein, however they may be referenced here for example purposes. Danger NEVER include any passwords or secrets, details of access mechanism, or intellectual property. If you think it shouldn't be seen by a baddy, then don't put it in the playbook!","title":"Engineering"},{"location":"6.-Engineering/Managing-Technical-Debt/","text":"","title":"Managing Technical Debt"},{"location":"6.-Engineering/Test-Environments/","text":"Who is this for? \u00b6 This page documents our test environments and the process a Feature/Change follows from being developed to being released to Live. These practices will be being reviewed and updated rapidly as we work to reduce dependencies and blockers in the development, test, and deployment cycle. Guiding Principles \u00b6 Note There is no inherent link between test environment type and development lifecycle stage. We must avoid principles such as \"QA is for testing, then it goes to stage, before it can go live\". We must ensure we perform good testing, but should not be constrained by imagined barriers. The many \"QA environments\" can be used for any type of testing, include PO The single \"Stage\" environment is used for Business Acceptance Testing (BAT) only. This is because BAT has specific test data in it at present, so the QA environments are not sufficient Turn on and off your QA environments when you start and finish using them - they cost money to keep on! Always ensure every component (i.e. each microservice) within a QA environment has the latest main branch (i.e. live) deployed to it before deploying the feature(s) under test. Test Environment Overview \u00b6 We currently have 6 QA environments (namely QA & QA-1 thru 5) and one Stage environment (Note: We plan to soon have 10 QA environments \u2013 QA & QA-1 thru 9). The QA environments are predominately used by the Delivery Team to test the Feature/Change The Stage environment is used by Stakeholders for BAT Only environments that are currently in use are ever switched on Deploying to Test Environments \u00b6 Delivery Team Testing \u00b6 A Feature/Change has been developed and is ready for testing - 1. Switch on a QA environment 1. Deploy the latest microservices along with the Feature/Change under test to the QA environment 1. Test the Feature/Change 1. Any defects raised at this phase are fixed and released to same QA environment * On release, ensure the latest microservices are deployed 1. On completion of this test phase, check all deployed microservices are the same version as Live * If the microservices remain the latest, the Feature/Change can move to the next phase i.e. PO Sign-Off * If the microservices no longer match Live: * Deploy the latest versions to the QA environment * Smoke Test * Move to the next phase i.e. PO Sign-Off For further information on the release process, please see [Deploying to Test Environments] in the Knowledge Base PO Sign-Off \u00b6 PO Sign-Off takes place on the QA environment with the latest versions of the microservices deployed (i.e. as close to Live as possible) Ideally, PO will be able to sign-off the release promptly to prevent further smoke testing should the microservices fall out of sync with Live. Any defects raised at this phase are fixed and released to same QA environment On release, ensure the latest microservices are deployed Once PO Sign-Off is obtained, the next phase depends on whether BAT is required or not BAT Phase Not Required \u00b6 Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Demo & Live. BAT Phase Required \u00b6 Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Stage for BAT. Check no BAT session is currently taking place on the Stage environment Switch on the Stage environment Deploy the latest microservices along with the Feature/Change under test to the Stage environment Smoke test where required BAT can commence Following sign-off from Stakeholders, a smoke test may be required should the microservices no longer be up to date (i.e. release to Live occurred during this time). The Feature/Change is now ready to be released to Demo & Live. Following Release to Live \u00b6 On completion of the release: Follow the branching strategy guidelines to ensure the main branch is updated The relevant QA environment and Stage (if not being used by another BAT session) must be switched off WorkFlow Diagram \u00b6","title":"Test Environments"},{"location":"6.-Engineering/Test-Environments/#who-is-this-for","text":"This page documents our test environments and the process a Feature/Change follows from being developed to being released to Live. These practices will be being reviewed and updated rapidly as we work to reduce dependencies and blockers in the development, test, and deployment cycle.","title":"Who is this for?"},{"location":"6.-Engineering/Test-Environments/#guiding-principles","text":"Note There is no inherent link between test environment type and development lifecycle stage. We must avoid principles such as \"QA is for testing, then it goes to stage, before it can go live\". We must ensure we perform good testing, but should not be constrained by imagined barriers. The many \"QA environments\" can be used for any type of testing, include PO The single \"Stage\" environment is used for Business Acceptance Testing (BAT) only. This is because BAT has specific test data in it at present, so the QA environments are not sufficient Turn on and off your QA environments when you start and finish using them - they cost money to keep on! Always ensure every component (i.e. each microservice) within a QA environment has the latest main branch (i.e. live) deployed to it before deploying the feature(s) under test.","title":"Guiding Principles"},{"location":"6.-Engineering/Test-Environments/#test-environment-overview","text":"We currently have 6 QA environments (namely QA & QA-1 thru 5) and one Stage environment (Note: We plan to soon have 10 QA environments \u2013 QA & QA-1 thru 9). The QA environments are predominately used by the Delivery Team to test the Feature/Change The Stage environment is used by Stakeholders for BAT Only environments that are currently in use are ever switched on","title":"Test Environment Overview"},{"location":"6.-Engineering/Test-Environments/#deploying-to-test-environments","text":"","title":"Deploying to Test Environments"},{"location":"6.-Engineering/Test-Environments/#delivery-team-testing","text":"A Feature/Change has been developed and is ready for testing - 1. Switch on a QA environment 1. Deploy the latest microservices along with the Feature/Change under test to the QA environment 1. Test the Feature/Change 1. Any defects raised at this phase are fixed and released to same QA environment * On release, ensure the latest microservices are deployed 1. On completion of this test phase, check all deployed microservices are the same version as Live * If the microservices remain the latest, the Feature/Change can move to the next phase i.e. PO Sign-Off * If the microservices no longer match Live: * Deploy the latest versions to the QA environment * Smoke Test * Move to the next phase i.e. PO Sign-Off For further information on the release process, please see [Deploying to Test Environments] in the Knowledge Base","title":"Delivery Team Testing"},{"location":"6.-Engineering/Test-Environments/#po-sign-off","text":"PO Sign-Off takes place on the QA environment with the latest versions of the microservices deployed (i.e. as close to Live as possible) Ideally, PO will be able to sign-off the release promptly to prevent further smoke testing should the microservices fall out of sync with Live. Any defects raised at this phase are fixed and released to same QA environment On release, ensure the latest microservices are deployed Once PO Sign-Off is obtained, the next phase depends on whether BAT is required or not","title":"PO Sign-Off"},{"location":"6.-Engineering/Test-Environments/#bat-phase-not-required","text":"Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Demo & Live.","title":"BAT Phase Not Required"},{"location":"6.-Engineering/Test-Environments/#bat-phase-required","text":"Testing is complete and PO have signed off the Feature/Change against the latest microservices. The Feature/Change is now ready to be released to Stage for BAT. Check no BAT session is currently taking place on the Stage environment Switch on the Stage environment Deploy the latest microservices along with the Feature/Change under test to the Stage environment Smoke test where required BAT can commence Following sign-off from Stakeholders, a smoke test may be required should the microservices no longer be up to date (i.e. release to Live occurred during this time). The Feature/Change is now ready to be released to Demo & Live.","title":"BAT Phase Required"},{"location":"6.-Engineering/Test-Environments/#following-release-to-live","text":"On completion of the release: Follow the branching strategy guidelines to ensure the main branch is updated The relevant QA environment and Stage (if not being used by another BAT session) must be switched off","title":"Following Release to Live"},{"location":"6.-Engineering/Test-Environments/#workflow-diagram","text":"","title":"WorkFlow Diagram"},{"location":"6.-Engineering/Peer-Reviewing/","text":"Who is this for? \u00b6 Outlines the mechanisms by which engineers share their approach to a specific solution during development. Engineers - dev & test Overview \u00b6 The purpose of a peer review is to identify mistakes as early as possible and ensure that the way a solution is implemented is: 1. Understood by more than one member of the team - creating a shared understanding of our platform 1. Inline with our quality standards Peer reviewing must not be left until last minute, or performed as a one-time event. Instead, we should be working closely with our colleagues to share our approach throughout the development cycle. This allows for early feedback around the way a solution is built, and allows for the reviewer to gain a greater degree of context. The final stage of peer reviewing is when code is merged into a parent branch (e.g. Work Branch --> Feature Branch, or Feature Branch --> Release Branch). This process allows for in depth reviewing of the full solution. Pair Programming & WIP Reviewing \u00b6 Think about potential review points as a team during sprint planning! Creating discrete review tasks will help track progress and remind the team of the need to review. Pair programming, screen share walk throughs, or simply reviewing a work branch are some ways to perform peer reviews of WIP. Any member of the team can perform a peer review for anyone else, not just a more senior member! Providing Feedback for WIP \u00b6 Feedback should be given directly to the original engineer by the reviewer. The reviewer should feel free to ask questions and provide their honest perspective. It is wise to support feedback with rationale; in what way could the solution be improved? how will these improvements help us improve quality? The original engineer is not obliged to act upon all feedback, but an open discussion will be very beneficial. Reviews whilst pair programming \u00b6 A review should still be performed for any code written during a pair programming session. However, it may be helpful for the two engineers to perform the review together at the end of the pairing session. E.g. the engineer that has been screen sharing/writing the code should check everything in, and then the other engineer lead the review. However both engineers would be encouraged to discuss whether or not the code they have written together fulfils the guidelines.= Submitting code for a review \u00b6 It is best to submit code to be reviewed frequently, both to get early feedback and to ensure that the reviewing engineer can get a good picture of the changes without being overwhelmed by a huge amount of changes all at once. In other words, follow the principle of little and often! When submitting a PR the engineer should follow the Pull Request Guidelines , and should ensure to provide some context of what the code being reviewed is try to achieve and how it is structured. If refactoring existing code (in preparation for subsequent changes), the refactoring should be performed separately and submitted for review in isolation. It would otherwise be much harder for the reviewing engineer to understand how the refactoring has been done if there are also additional changes and new functionality mixed in.","title":"Peer Reviewing"},{"location":"6.-Engineering/Peer-Reviewing/#who-is-this-for","text":"Outlines the mechanisms by which engineers share their approach to a specific solution during development. Engineers - dev & test","title":"Who is this for?"},{"location":"6.-Engineering/Peer-Reviewing/#overview","text":"The purpose of a peer review is to identify mistakes as early as possible and ensure that the way a solution is implemented is: 1. Understood by more than one member of the team - creating a shared understanding of our platform 1. Inline with our quality standards Peer reviewing must not be left until last minute, or performed as a one-time event. Instead, we should be working closely with our colleagues to share our approach throughout the development cycle. This allows for early feedback around the way a solution is built, and allows for the reviewer to gain a greater degree of context. The final stage of peer reviewing is when code is merged into a parent branch (e.g. Work Branch --> Feature Branch, or Feature Branch --> Release Branch). This process allows for in depth reviewing of the full solution.","title":"Overview"},{"location":"6.-Engineering/Peer-Reviewing/#pair-programming-wip-reviewing","text":"Think about potential review points as a team during sprint planning! Creating discrete review tasks will help track progress and remind the team of the need to review. Pair programming, screen share walk throughs, or simply reviewing a work branch are some ways to perform peer reviews of WIP. Any member of the team can perform a peer review for anyone else, not just a more senior member!","title":"Pair Programming &amp; WIP Reviewing"},{"location":"6.-Engineering/Peer-Reviewing/#providing-feedback-for-wip","text":"Feedback should be given directly to the original engineer by the reviewer. The reviewer should feel free to ask questions and provide their honest perspective. It is wise to support feedback with rationale; in what way could the solution be improved? how will these improvements help us improve quality? The original engineer is not obliged to act upon all feedback, but an open discussion will be very beneficial.","title":"Providing Feedback for WIP"},{"location":"6.-Engineering/Peer-Reviewing/#reviews-whilst-pair-programming","text":"A review should still be performed for any code written during a pair programming session. However, it may be helpful for the two engineers to perform the review together at the end of the pairing session. E.g. the engineer that has been screen sharing/writing the code should check everything in, and then the other engineer lead the review. However both engineers would be encouraged to discuss whether or not the code they have written together fulfils the guidelines.=","title":"Reviews whilst pair programming"},{"location":"6.-Engineering/Peer-Reviewing/#submitting-code-for-a-review","text":"It is best to submit code to be reviewed frequently, both to get early feedback and to ensure that the reviewing engineer can get a good picture of the changes without being overwhelmed by a huge amount of changes all at once. In other words, follow the principle of little and often! When submitting a PR the engineer should follow the Pull Request Guidelines , and should ensure to provide some context of what the code being reviewed is try to achieve and how it is structured. If refactoring existing code (in preparation for subsequent changes), the refactoring should be performed separately and submitted for review in isolation. It would otherwise be much harder for the reviewing engineer to understand how the refactoring has been done if there are also additional changes and new functionality mixed in.","title":"Submitting code for a review"},{"location":"6.-Engineering/Peer-Reviewing/Performing-a-Peer-Review/","text":"The engineer performing the review must: Copy and paste the template checklist (below) into their a new comment on the PR in GitHub Start reviewing the changes with respect to the Quality Standards When the reviewer is happy that a particular guideline has been met, the relevant item in the checklist is checked If any guidelines are not met, the reviewer should add a comment (either a general comment, or next to the offending portion of code) to explain why When the reviewer has finished reviewing the code, they must submit the review via GitHub: * If all the guidelines have all been met, they should tick the Approve option * Otherwise, they should tick the Request Changes option Engineers are also encouraged to provide positive feedback. If code being submitted for review is well-structured and meets all guidelines, then this should be celebrated. Checklist Template \u00b6 - [ ] Code implements the backlog item - [ ] Code is readable and well-structured - [ ] Code adheres to SOLID design principles - [ ] Code is not duplicated, or is moved into shared packages where relevant - [ ] Endpoint paths are restful and follow guidelines - [ ] Avoid hardcoded variables - all behaviour should be configurable via configuration settings - [ ] Code has good logging throughout - [ ] Code is well unit-tested - [ ] Minimum coverage level of approx 80% is maintained - [ ] Code coverage has been increased where it is not already at approx 80% - [ ] New UI elements are Accessible - [ ] Code follows Security best-practices - [ ] Code follows Performance best-practices - [ ] Documentation has been created or updated","title":"Performing a Peer Review"},{"location":"6.-Engineering/Peer-Reviewing/Performing-a-Peer-Review/#checklist-template","text":"- [ ] Code implements the backlog item - [ ] Code is readable and well-structured - [ ] Code adheres to SOLID design principles - [ ] Code is not duplicated, or is moved into shared packages where relevant - [ ] Endpoint paths are restful and follow guidelines - [ ] Avoid hardcoded variables - all behaviour should be configurable via configuration settings - [ ] Code has good logging throughout - [ ] Code is well unit-tested - [ ] Minimum coverage level of approx 80% is maintained - [ ] Code coverage has been increased where it is not already at approx 80% - [ ] New UI elements are Accessible - [ ] Code follows Security best-practices - [ ] Code follows Performance best-practices - [ ] Documentation has been created or updated","title":"Checklist Template"},{"location":"6.-Engineering/Quality-Standards/","text":"Who is this for? \u00b6 Outlines the standards to which Software Engineers and Test Engineers must adhere when writing code. Overview \u00b6 Quality means more than simply the way code is structured! Great quality solutions cover many facets, including: Readability & Maintainability Security & Compliance Robustness & Performance Architectural integrity Cost & environmentally friendly This is why it is important to share our approach to solutions early! Peer reviews should be looking to improve any/all of these areas, and it may often be too late by the time you have working code ready to merge. Guiding Principles \u00b6 Code must be easy to read and understand C# Code must follow OO patterns and SOLID principles Coding and naming conventions must be followed Unit and integrations test coverage must be maintained or improved Event logging must be used to provide useful diagnostic information Code must be self describing, and must not rely on code commenting to demystify chaos Code must be well documented, and must use code commenting and READMEs to enable rapid discovery User interfaces must be accessible and conform to WCAG AA Code must be secure and follow industry recommended practices Code must be performant, and able to handle live-like datasets and load Code should be green Guidelines for high-quality code \u00b6 Readable and Maintainable \u00b6 First and foremost, high-quality code must be easy to read and understand. Code must also follow our in-house coding conventions . The names of methods and classes etc. must clearly indicate what they do, and methods and classes must not be too lengthy. Code must also follow the SOLID principles , which will help to make it better structured, more readable and keep methods and classes manageable sizes. When creating new endpoints on microservices, ensure that they follow the endpoint naming conventions . Making code more maintainable is effectively the practice of minimising the amount of times you have to update it. Avoid using hardcoded values in code, as these values can only be changed by changing the code. Instead, consider making the behaviour of the code configurable by offloading these values to configuration. Similarly, do not repeat yourself (DRY)! Avoid copy/pasting code; instead consider how and where to share it. For smaller snippets of re-usable code, consider putting it in a helper class or extension method. Also consider the scope of any shared code; is it specific to the current repository, or is it more general code that might benefit from living in a shared code library for re-use across other repositories too? Following the Single Responsibility Principle (one of the SOLID principles, see above) should help with this. Do not commit commented-out code. It makes code less readable, and also causes confusion. When commented-out code is committed to a repository, it is not clear whether it should simply be deleted, or the code needs to be adapted and integrated, i.e. is it a cryptic TODO? Unit & Integration Tests \u00b6 Code must be well covered by unit & integration tests . Unit & integration testing not only increases our chances of catching bugs, increases engineer confidence in any changes being made, but is also an indication that the code is well-structured. To meet quality guidelines, all .NET code must be covered by Unit and/or integration tests with the aim of achieving 80% code coverage. For legacy repositories that do not meet this threshold, the aim should be to increase the coverage coverage percentage when writing new code, so as to incrementally meet the desired threshold. For repositories that are already well-tested, all new code must be sufficiently well tested so as to not bring the average coverage down. NOTE: code coverage is not a perfect metric. It is entirely possible to write poor tests that achieve an arbitrary coverage without providing any of the real benefits. Code repositories must measure and report on code coverage metrics as part of Continuous Integration. If not already present, add the code coverage testing tools to the repository being worked on. As engineers write tests for a repository, they must ensure the tests.yml file is updated with the latest coverage level. Observability & Logging \u00b6 Building observable systems enables development engineers to measure how well the application is behaving. Observability serves the following goals: Provide holistic view of the application health. Help measure business performance for the customer. Measure operational performance of the system. Identify and diagnose failures to get to the problem fast. A critical part of this is ensuring that services are logging useful events and errors. Log messages need to contain enough information to help an engineer understand what was happening, including for example Website Key or Client Key, ID of an asset or document affected, action being performed etc. See more information about logging in Quartex , and further reading on general best practices for logging. Documentation \u00b6 Every software development project requires documentation. Agile Software Development values working software over comprehensive documentation . However, repositories must include the key information needed to understand the development and the use of the generated software. Good documentation should work towards these goals: Facilitate the onboarding of new team members Improve communication and collaboration between teams Enable other engineers to successfully work with the software Every repository must have a README, which should succinctly explain: The purpose of the repository For a service, this boils down to \"what is the service responsible for?\" Otherwise explain the use-cases for using or modifying the repository Any external dependencies that are not automatically imported Additional steps required to build or debug the code Any processes that are unique to this repository Further reading on documentation guidelines . Documentation must never include any passwords or other sensitive configuration. These should be stored and documented in a secure manner, such as in a Password Manager ( TODO - setup password management for division ) or in a Secrets Manager. Any hacks or shortcuts or TODOs must be documented, as these all represent the addition of different types of technical debt. Engineers must be mindful of any technical debt being committed, and must have a plan to resolve it. This might include creating a backlog item to address the technical debt, with a plan to bring it into a subsequent sprint. Accessibility \u00b6 Any new UI elements must be made accessible, and conform to WCAG AA. This includes making the UI usable via Screen Reader, including adding aria tags where relevant. The UI must also be usable with the keyboard. Secure Code \u00b6 Engineers must ensure that code adheres to industry-recommended standard practices for secure design and implementation of code. For practical purposes, engineers must be familiar with the OWASP top 10 vulnerabilities . The OWASP secure coding practices guide is also a useful reference. Ensuring software is secure relies not just on the software being written, but also the Frameworks and Libraries it depends on; which makes keeping these updated is important. Secure software also needs to be run on infrastructure, which must itself be up-to-date and configured securely. When writing and committing code, engineers must ensure they are never commit secrets (i.e database passwords, access keys or other sensitive config) into source control or documentation. New endpoints and pages must have the correct authentication and authorisation checks in place. Package Dependency Management \u00b6 When including packages/dependencies (internal or external) developers must not use a floating package version, e.g. : - Do: - Node.js: \"dependencies\": { \"ExamplePackage\": \"2.0.1\" } - C#: <PackageReference Include=\"ExamplePackage\" version=\"2.0.1\" /> - Don't: - Node.js: \"dependencies\": { \"ExamplePackage\": \"^2.0.1\" } - C#: <PackageReference Include=\"ExamplePackage\" version=\"2.0.*\" /> Performant Code \u00b6 Engineers must ensure the code they write is performant. This refers both to ensuring individual actions and pages are loaded within an acceptable timeframe, and ensuring that functionality does not degrade with large datasets. In particular it is important to perform testing with live-like datasets to ensure the software will behave as expected. NOTE: specific non-functional requirements for acceptable performance will be defined at a later date. The Microsoft article on .NET Core performance best practices is very helpful. General performance tips \u00b6 Use caching where ever it is possible and appropriate. The Quartex caching guidelines contain mechanisms for a range of different types of caching, including the caching service calls for retrieving data, and output caching to cache entire pages. The fastest way of doing work is to not have to do the work at all! Use async / await calls wherever possible. The async pattern allows the operatic system to assign idle CPU capacity to a thread that has work to do, instead of keeping the thread busy. Whilst it may be difficult to see much of a difference when developing locally, using async / await allows a webserver to handle many more requests in parallel. Entity Framework tips \u00b6 When retrieving data from databases, use .Include() calls only where strictly necessary. Whilst it might be quite convenient, it is likely that with a large dataset, the increased amount of data returned will have a performance impact which may not be apparent when developing locally. This simply further re-enforces the need to test with live-like datasets. Avoid using queries that Entity Framework will not be able to convert to SQL. A common mistake is to use .ToLowerInvariant() in a .Where() clause to perform a case-insensitive search. In these scenarios, Entity Framework will translate as much as possible into SQL, but then retrieve the rest of the data set and perform the rest of the query in memory - potentially resulting in a huge dataset being retrieved from the database. E.g. consider the following fictitious query which retrieves an asset that matches a given path, where deleted is false. var children = _unitOfWork . Assets . Where ( a => a . Deleted == false && a . Path . ToLowerInvariant () == pathToSearchFor ); In this example, since EF is unable to translate .ToLowerInvariant() into SQL, it will effectively run a query like select * from Assets where Deleted=0 and then run the Path.ToLowerInvariant() == pathToSearchFor comparison in memory, on every single returned row. For a very large database, this sort of subtlety can be cripplingly slow!","title":"Quality Standards"},{"location":"6.-Engineering/Quality-Standards/#who-is-this-for","text":"Outlines the standards to which Software Engineers and Test Engineers must adhere when writing code.","title":"Who is this for?"},{"location":"6.-Engineering/Quality-Standards/#overview","text":"Quality means more than simply the way code is structured! Great quality solutions cover many facets, including: Readability & Maintainability Security & Compliance Robustness & Performance Architectural integrity Cost & environmentally friendly This is why it is important to share our approach to solutions early! Peer reviews should be looking to improve any/all of these areas, and it may often be too late by the time you have working code ready to merge.","title":"Overview"},{"location":"6.-Engineering/Quality-Standards/#guiding-principles","text":"Code must be easy to read and understand C# Code must follow OO patterns and SOLID principles Coding and naming conventions must be followed Unit and integrations test coverage must be maintained or improved Event logging must be used to provide useful diagnostic information Code must be self describing, and must not rely on code commenting to demystify chaos Code must be well documented, and must use code commenting and READMEs to enable rapid discovery User interfaces must be accessible and conform to WCAG AA Code must be secure and follow industry recommended practices Code must be performant, and able to handle live-like datasets and load Code should be green","title":"Guiding Principles"},{"location":"6.-Engineering/Quality-Standards/#guidelines-for-high-quality-code","text":"","title":"Guidelines for high-quality code"},{"location":"6.-Engineering/Quality-Standards/#readable-and-maintainable","text":"First and foremost, high-quality code must be easy to read and understand. Code must also follow our in-house coding conventions . The names of methods and classes etc. must clearly indicate what they do, and methods and classes must not be too lengthy. Code must also follow the SOLID principles , which will help to make it better structured, more readable and keep methods and classes manageable sizes. When creating new endpoints on microservices, ensure that they follow the endpoint naming conventions . Making code more maintainable is effectively the practice of minimising the amount of times you have to update it. Avoid using hardcoded values in code, as these values can only be changed by changing the code. Instead, consider making the behaviour of the code configurable by offloading these values to configuration. Similarly, do not repeat yourself (DRY)! Avoid copy/pasting code; instead consider how and where to share it. For smaller snippets of re-usable code, consider putting it in a helper class or extension method. Also consider the scope of any shared code; is it specific to the current repository, or is it more general code that might benefit from living in a shared code library for re-use across other repositories too? Following the Single Responsibility Principle (one of the SOLID principles, see above) should help with this. Do not commit commented-out code. It makes code less readable, and also causes confusion. When commented-out code is committed to a repository, it is not clear whether it should simply be deleted, or the code needs to be adapted and integrated, i.e. is it a cryptic TODO?","title":"Readable and Maintainable"},{"location":"6.-Engineering/Quality-Standards/#unit-integration-tests","text":"Code must be well covered by unit & integration tests . Unit & integration testing not only increases our chances of catching bugs, increases engineer confidence in any changes being made, but is also an indication that the code is well-structured. To meet quality guidelines, all .NET code must be covered by Unit and/or integration tests with the aim of achieving 80% code coverage. For legacy repositories that do not meet this threshold, the aim should be to increase the coverage coverage percentage when writing new code, so as to incrementally meet the desired threshold. For repositories that are already well-tested, all new code must be sufficiently well tested so as to not bring the average coverage down. NOTE: code coverage is not a perfect metric. It is entirely possible to write poor tests that achieve an arbitrary coverage without providing any of the real benefits. Code repositories must measure and report on code coverage metrics as part of Continuous Integration. If not already present, add the code coverage testing tools to the repository being worked on. As engineers write tests for a repository, they must ensure the tests.yml file is updated with the latest coverage level.","title":"Unit &amp; Integration Tests"},{"location":"6.-Engineering/Quality-Standards/#observability-logging","text":"Building observable systems enables development engineers to measure how well the application is behaving. Observability serves the following goals: Provide holistic view of the application health. Help measure business performance for the customer. Measure operational performance of the system. Identify and diagnose failures to get to the problem fast. A critical part of this is ensuring that services are logging useful events and errors. Log messages need to contain enough information to help an engineer understand what was happening, including for example Website Key or Client Key, ID of an asset or document affected, action being performed etc. See more information about logging in Quartex , and further reading on general best practices for logging.","title":"Observability &amp; Logging"},{"location":"6.-Engineering/Quality-Standards/#documentation","text":"Every software development project requires documentation. Agile Software Development values working software over comprehensive documentation . However, repositories must include the key information needed to understand the development and the use of the generated software. Good documentation should work towards these goals: Facilitate the onboarding of new team members Improve communication and collaboration between teams Enable other engineers to successfully work with the software Every repository must have a README, which should succinctly explain: The purpose of the repository For a service, this boils down to \"what is the service responsible for?\" Otherwise explain the use-cases for using or modifying the repository Any external dependencies that are not automatically imported Additional steps required to build or debug the code Any processes that are unique to this repository Further reading on documentation guidelines . Documentation must never include any passwords or other sensitive configuration. These should be stored and documented in a secure manner, such as in a Password Manager ( TODO - setup password management for division ) or in a Secrets Manager. Any hacks or shortcuts or TODOs must be documented, as these all represent the addition of different types of technical debt. Engineers must be mindful of any technical debt being committed, and must have a plan to resolve it. This might include creating a backlog item to address the technical debt, with a plan to bring it into a subsequent sprint.","title":"Documentation"},{"location":"6.-Engineering/Quality-Standards/#accessibility","text":"Any new UI elements must be made accessible, and conform to WCAG AA. This includes making the UI usable via Screen Reader, including adding aria tags where relevant. The UI must also be usable with the keyboard.","title":"Accessibility"},{"location":"6.-Engineering/Quality-Standards/#secure-code","text":"Engineers must ensure that code adheres to industry-recommended standard practices for secure design and implementation of code. For practical purposes, engineers must be familiar with the OWASP top 10 vulnerabilities . The OWASP secure coding practices guide is also a useful reference. Ensuring software is secure relies not just on the software being written, but also the Frameworks and Libraries it depends on; which makes keeping these updated is important. Secure software also needs to be run on infrastructure, which must itself be up-to-date and configured securely. When writing and committing code, engineers must ensure they are never commit secrets (i.e database passwords, access keys or other sensitive config) into source control or documentation. New endpoints and pages must have the correct authentication and authorisation checks in place.","title":"Secure Code"},{"location":"6.-Engineering/Quality-Standards/#package-dependency-management","text":"When including packages/dependencies (internal or external) developers must not use a floating package version, e.g. : - Do: - Node.js: \"dependencies\": { \"ExamplePackage\": \"2.0.1\" } - C#: <PackageReference Include=\"ExamplePackage\" version=\"2.0.1\" /> - Don't: - Node.js: \"dependencies\": { \"ExamplePackage\": \"^2.0.1\" } - C#: <PackageReference Include=\"ExamplePackage\" version=\"2.0.*\" />","title":"Package Dependency Management"},{"location":"6.-Engineering/Quality-Standards/#performant-code","text":"Engineers must ensure the code they write is performant. This refers both to ensuring individual actions and pages are loaded within an acceptable timeframe, and ensuring that functionality does not degrade with large datasets. In particular it is important to perform testing with live-like datasets to ensure the software will behave as expected. NOTE: specific non-functional requirements for acceptable performance will be defined at a later date. The Microsoft article on .NET Core performance best practices is very helpful.","title":"Performant Code"},{"location":"6.-Engineering/Quality-Standards/#general-performance-tips","text":"Use caching where ever it is possible and appropriate. The Quartex caching guidelines contain mechanisms for a range of different types of caching, including the caching service calls for retrieving data, and output caching to cache entire pages. The fastest way of doing work is to not have to do the work at all! Use async / await calls wherever possible. The async pattern allows the operatic system to assign idle CPU capacity to a thread that has work to do, instead of keeping the thread busy. Whilst it may be difficult to see much of a difference when developing locally, using async / await allows a webserver to handle many more requests in parallel.","title":"General performance tips"},{"location":"6.-Engineering/Quality-Standards/#entity-framework-tips","text":"When retrieving data from databases, use .Include() calls only where strictly necessary. Whilst it might be quite convenient, it is likely that with a large dataset, the increased amount of data returned will have a performance impact which may not be apparent when developing locally. This simply further re-enforces the need to test with live-like datasets. Avoid using queries that Entity Framework will not be able to convert to SQL. A common mistake is to use .ToLowerInvariant() in a .Where() clause to perform a case-insensitive search. In these scenarios, Entity Framework will translate as much as possible into SQL, but then retrieve the rest of the data set and perform the rest of the query in memory - potentially resulting in a huge dataset being retrieved from the database. E.g. consider the following fictitious query which retrieves an asset that matches a given path, where deleted is false. var children = _unitOfWork . Assets . Where ( a => a . Deleted == false && a . Path . ToLowerInvariant () == pathToSearchFor ); In this example, since EF is unable to translate .ToLowerInvariant() into SQL, it will effectively run a query like select * from Assets where Deleted=0 and then run the Path.ToLowerInvariant() == pathToSearchFor comparison in memory, on every single returned row. For a very large database, this sort of subtlety can be cripplingly slow!","title":"Entity Framework tips"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/","text":"[[ TOC ]] C# Coding Conventions \u00b6 Our coding standards use the Microsoft C# conventions . All engineers must use and be familiar with them. Additional in-house conventions \u00b6 Controllers (and other code entry points like Background Tasks and Message Queue Message Handlers which are similar to controllers) should be kept as lean as possible. Each controller should dependency inject a Service that is responsible for business logic. - The controller should do some basic input validation (returning an appropriate error code if validation fails) and then call the service to perform the bulk of the work. - The main service can then inject as many other classes as it needs to perform its function. Smaller repeated snippets of code should be refactored out and made re-usable, using one of the following approaches: - Simpler pieces of code can be grouped into static methods within a SomethingHelper class - Extension methods should generally be grouped together based on the class they extend, and be called MyClassExtensions - Consider whether any refactored code could be useful moved into a shared package for re-use in other repositories For constructing a small string, use String Interpolation (i.e. string formatted = $\"My variable is {variable}\" ) rather than string.Format() . However if you need to log the value of a string, remember to use Serilog logging properties. For constructing a larger string e.g. within a loop, use StringBuilder instead of concatenating many strings together. Unit and Integration tests should make use Shouldly instead of Assert.That to make assertions more readable. TypeScript and JavaScript Coding Conventions \u00b6 TODO: document this section and link to an industry-standard style guide","title":"Coding Conventions"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/#c-coding-conventions","text":"Our coding standards use the Microsoft C# conventions . All engineers must use and be familiar with them.","title":"C# Coding Conventions"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/#additional-in-house-conventions","text":"Controllers (and other code entry points like Background Tasks and Message Queue Message Handlers which are similar to controllers) should be kept as lean as possible. Each controller should dependency inject a Service that is responsible for business logic. - The controller should do some basic input validation (returning an appropriate error code if validation fails) and then call the service to perform the bulk of the work. - The main service can then inject as many other classes as it needs to perform its function. Smaller repeated snippets of code should be refactored out and made re-usable, using one of the following approaches: - Simpler pieces of code can be grouped into static methods within a SomethingHelper class - Extension methods should generally be grouped together based on the class they extend, and be called MyClassExtensions - Consider whether any refactored code could be useful moved into a shared package for re-use in other repositories For constructing a small string, use String Interpolation (i.e. string formatted = $\"My variable is {variable}\" ) rather than string.Format() . However if you need to log the value of a string, remember to use Serilog logging properties. For constructing a larger string e.g. within a loop, use StringBuilder instead of concatenating many strings together. Unit and Integration tests should make use Shouldly instead of Assert.That to make assertions more readable.","title":"Additional in-house conventions"},{"location":"6.-Engineering/Quality-Standards/Coding-Conventions/#typescript-and-javascript-coding-conventions","text":"TODO: document this section and link to an industry-standard style guide","title":"TypeScript and JavaScript Coding Conventions"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/","text":"The following guidelines are adapted from the REST API Tutorial . Endpoint path conventions \u00b6 All our microservice endpoints should start with a version (e.g. v1 , v2 etc.), which will have different uses depending on the context. Most microservice endpoints will also relate directly either to a specific Client or specific Website ; although some microservices don't require this (e.g. the File Validation service): /{version}/{ClientKey}/rest/of/url - Version and ClientKey /{version}/{WebsiteKey}/rest/of/url - Version and WebsiteKey /{version}/rest/of/url - Version only Requests paths should use plural nouns, and indicate the hierarchy of the objects that they represent as much as possible. It can be helpful to think of every endpoint path as representing a specific object (or a specific set of objects). /{version}/{client}/jobs - Get all Jobs /{version}/{client}/jobs/{JobId} - Get details for a single Job /{version}/{client}/jobs/{JobId}/items - Get all the Items for a single Job /{version}/{client}/jobs/{JobId}/items/{ItemId} - Get details for a single Item within a given Job NOTE : avoid using verbs in endpoint paths wherever possible. E.g. don't use getasset/{assetId} because the get is already in the HTTP verb. This leads nicely onto the following section... Use the relevant HTTP Verb for each action. \u00b6 Use GET requests for reading data POST requests should be creating new items or uploading files PUT requests should be used for updating existing items DELETE requests should be used for deleting data If we consider each endpoint's path to be a specific object, then it makes sense that each HTTP verb is performing the relevant action on that object. E.g. a GET request will retrieve the object, a PUT request will modify the same object, and a DELETE request would delete that same object. For GET requests, only identifiers (e.g. numerical IDs or Paths) should go in to the URL; filters, sorting or pagination (i.e. skip and take ) parameters should be included as Query String values (implement the IQueryStringRequest interface). In extreme cases it might be necessary to use a POST request to include all the required parameters but avoid doing this if at all possible as it strictly speaking the wrong verb. Some examples below: /{version}/{client}/assets/{skip}/{take} - instead use /{version}/{client}/assets?skip={skip}&take={take} In principle, there could be several identical looking URLs - but using different HTTP Verbs would perform different actions. E.g. GET /{version}/{client}/jobs - Get all jobs. This may include pagination or filtering as query string GET /{version}/{client}/jobs/{JobId} - Get details for a specific job PUT /{version}/{client}/jobs/{JobId} - Update a job (e.g. set its status to Complete - the exact properties to be updated would live in the request body) DELETE /{version}/{client}/jobs/{JobId} - Delete the job Wherever possible avoid putting an action into the URL if it can be indicated by the HTTP Verb. The only exception to this is where there are multiple similar actions that would otherwise need identical URLs. For example: POST /{version}/{client}/assets/deleteasset/{id} - this is very bad practice, instead use the below DELETE /{version}/{client}/assets/{id} - this performs a soft delete on the asset with the specified id DELETE /{version}/{client}/assets/{id}/_force - this performs a hard delete on the same asset The last case is effectively a variation on the previous action, so adding an extra path component for differentiation is acceptable. Use the convention of adding an underscore to signal this variation. Filtering and pagination \u00b6 Wherever possible, a GET request should always be used for reading data. If an endpoint represents a collection of objects, (e.g. /{version}/{client}/assets might represent all Assets for a client), then use query string parameters to allow the caller to retrieve the objects they need. Use ?skip and ?take parameters to allow the caller to perform pagination Use ?sortBy and ?sortDir parameters to allow the caller to specify sort ordering Provide whatever parameters make sense in the context of the use-case to allow filtering. E.g. you might provide ?collections=A,B to filter by collection A or collection B ?status=InComplete,ForReview to filter by assets where the status is Incomplete for For Review A POST request should only ever be used in the case where the maximum length of a query string is insufficient to supply the complexity of the parameters. Make API Endpoints as generic as possible \u00b6 Try to avoid having multiple endpoints that do similar things. E.g. instead of having multiple endpoints to get a different set of assets for different purposes, instead extend the existing endpoint to provide more flexibility. E.g. assume we have the following existing endpoint that is already used for Manage Assets: GET /{version}/{client}/assets - Assumes filtering and pagination are provided by Query String parameters Imagine then that someone needed to get a list of assets for promotion to a website; the example above might not quite meet their needs as it includes metadata-only records for example, so they decide to create a new endpoint specific to their use-case. GET /{version}/{client}/assets/for-promotion/{WebsiteKey} However this isn't ideal, as it creates more code that needs to be maintained and tested. Instead they could simply extend the existing endpoint to add a ?includeMetadataOnly=false query string value to make sure the existing endpoint works for them too.","title":"Endpoint Naming Conventions"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#endpoint-path-conventions","text":"All our microservice endpoints should start with a version (e.g. v1 , v2 etc.), which will have different uses depending on the context. Most microservice endpoints will also relate directly either to a specific Client or specific Website ; although some microservices don't require this (e.g. the File Validation service): /{version}/{ClientKey}/rest/of/url - Version and ClientKey /{version}/{WebsiteKey}/rest/of/url - Version and WebsiteKey /{version}/rest/of/url - Version only Requests paths should use plural nouns, and indicate the hierarchy of the objects that they represent as much as possible. It can be helpful to think of every endpoint path as representing a specific object (or a specific set of objects). /{version}/{client}/jobs - Get all Jobs /{version}/{client}/jobs/{JobId} - Get details for a single Job /{version}/{client}/jobs/{JobId}/items - Get all the Items for a single Job /{version}/{client}/jobs/{JobId}/items/{ItemId} - Get details for a single Item within a given Job NOTE : avoid using verbs in endpoint paths wherever possible. E.g. don't use getasset/{assetId} because the get is already in the HTTP verb. This leads nicely onto the following section...","title":"Endpoint path conventions"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#use-the-relevant-http-verb-for-each-action","text":"Use GET requests for reading data POST requests should be creating new items or uploading files PUT requests should be used for updating existing items DELETE requests should be used for deleting data If we consider each endpoint's path to be a specific object, then it makes sense that each HTTP verb is performing the relevant action on that object. E.g. a GET request will retrieve the object, a PUT request will modify the same object, and a DELETE request would delete that same object. For GET requests, only identifiers (e.g. numerical IDs or Paths) should go in to the URL; filters, sorting or pagination (i.e. skip and take ) parameters should be included as Query String values (implement the IQueryStringRequest interface). In extreme cases it might be necessary to use a POST request to include all the required parameters but avoid doing this if at all possible as it strictly speaking the wrong verb. Some examples below: /{version}/{client}/assets/{skip}/{take} - instead use /{version}/{client}/assets?skip={skip}&take={take} In principle, there could be several identical looking URLs - but using different HTTP Verbs would perform different actions. E.g. GET /{version}/{client}/jobs - Get all jobs. This may include pagination or filtering as query string GET /{version}/{client}/jobs/{JobId} - Get details for a specific job PUT /{version}/{client}/jobs/{JobId} - Update a job (e.g. set its status to Complete - the exact properties to be updated would live in the request body) DELETE /{version}/{client}/jobs/{JobId} - Delete the job Wherever possible avoid putting an action into the URL if it can be indicated by the HTTP Verb. The only exception to this is where there are multiple similar actions that would otherwise need identical URLs. For example: POST /{version}/{client}/assets/deleteasset/{id} - this is very bad practice, instead use the below DELETE /{version}/{client}/assets/{id} - this performs a soft delete on the asset with the specified id DELETE /{version}/{client}/assets/{id}/_force - this performs a hard delete on the same asset The last case is effectively a variation on the previous action, so adding an extra path component for differentiation is acceptable. Use the convention of adding an underscore to signal this variation.","title":"Use the relevant HTTP Verb for each action."},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#filtering-and-pagination","text":"Wherever possible, a GET request should always be used for reading data. If an endpoint represents a collection of objects, (e.g. /{version}/{client}/assets might represent all Assets for a client), then use query string parameters to allow the caller to retrieve the objects they need. Use ?skip and ?take parameters to allow the caller to perform pagination Use ?sortBy and ?sortDir parameters to allow the caller to specify sort ordering Provide whatever parameters make sense in the context of the use-case to allow filtering. E.g. you might provide ?collections=A,B to filter by collection A or collection B ?status=InComplete,ForReview to filter by assets where the status is Incomplete for For Review A POST request should only ever be used in the case where the maximum length of a query string is insufficient to supply the complexity of the parameters.","title":"Filtering and pagination"},{"location":"6.-Engineering/Quality-Standards/Endpoint-Naming-Conventions/#make-api-endpoints-as-generic-as-possible","text":"Try to avoid having multiple endpoints that do similar things. E.g. instead of having multiple endpoints to get a different set of assets for different purposes, instead extend the existing endpoint to provide more flexibility. E.g. assume we have the following existing endpoint that is already used for Manage Assets: GET /{version}/{client}/assets - Assumes filtering and pagination are provided by Query String parameters Imagine then that someone needed to get a list of assets for promotion to a website; the example above might not quite meet their needs as it includes metadata-only records for example, so they decide to create a new endpoint specific to their use-case. GET /{version}/{client}/assets/for-promotion/{WebsiteKey} However this isn't ideal, as it creates more code that needs to be maintained and tested. Instead they could simply extend the existing endpoint to add a ?includeMetadataOnly=false query string value to make sure the existing endpoint works for them too.","title":"Make API Endpoints as generic as possible"},{"location":"6.-Engineering/Quality-Standards/Exceptions-Best-Practices/","text":"Fundamentals \u00b6 Read and be familiar with the following: Exception Fundamentals Using Exceptions Fundamentals Exception Handling Fundamentals Creating and Throwing Exceptions Fundamentals Compiler-generated Exceptions Fundamentals Best Practices for Exceptions Using Standard Exception Types Best Practises \u00b6 Engineers should follow Microsoft Exception Best Practises. Engineers should not throw System.Exception, System.SystemException, System.NullReferenceException, or System.IndexOutOfRangeException in their own code. Engineers should throw predefined Exceptions when it is suitable to do so e.g. ArgumentNullException Engineers should only create Exceptions when predefined ones do not exist, but should priorities specificity over a less detailed generic predefined exception. e.g. Create custom OutOfMoneyException instead of InvalidOperationException(\"OutOfMoney\") Engineers should only catch specific types of exceptions not a general Exception catch. Engineers should use finally blocks to clean up resources even if no exceptions have been caught.","title":"Fundamentals"},{"location":"6.-Engineering/Quality-Standards/Exceptions-Best-Practices/#fundamentals","text":"Read and be familiar with the following: Exception Fundamentals Using Exceptions Fundamentals Exception Handling Fundamentals Creating and Throwing Exceptions Fundamentals Compiler-generated Exceptions Fundamentals Best Practices for Exceptions Using Standard Exception Types","title":"Fundamentals"},{"location":"6.-Engineering/Quality-Standards/Exceptions-Best-Practices/#best-practises","text":"Engineers should follow Microsoft Exception Best Practises. Engineers should not throw System.Exception, System.SystemException, System.NullReferenceException, or System.IndexOutOfRangeException in their own code. Engineers should throw predefined Exceptions when it is suitable to do so e.g. ArgumentNullException Engineers should only create Exceptions when predefined ones do not exist, but should priorities specificity over a less detailed generic predefined exception. e.g. Create custom OutOfMoneyException instead of InvalidOperationException(\"OutOfMoney\") Engineers should only catch specific types of exceptions not a general Exception catch. Engineers should use finally blocks to clean up resources even if no exceptions have been caught.","title":"Best Practises"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/","text":"Dotnet Best Practise \u00b6 Engineers should use Log message template when generating log messages (See Here) TypeScript Best Practice \u00b6 Engineers should not use console.log directly, as this can be visible by end users. Alternatively we should be using the Tracer objects from the Eden.Scripts library. (See Monitoring Tips) General Best Practice \u00b6 Engineers should provided enough information engineers understand what was happening when reading the log e.g. Log the client key responsible for the call _logger.LogInformation(\"Create Controlled Vocabulary '{VocabName}' for client {clientKey}\", request.ControlledVocabulary.Name, clientKey); Engineers should use the correct log level when generating logs (see here)","title":"Logging Best Practise"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/#dotnet-best-practise","text":"Engineers should use Log message template when generating log messages (See Here)","title":"Dotnet Best Practise"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/#typescript-best-practice","text":"Engineers should not use console.log directly, as this can be visible by end users. Alternatively we should be using the Tracer objects from the Eden.Scripts library. (See Monitoring Tips)","title":"TypeScript Best Practice"},{"location":"6.-Engineering/Quality-Standards/Logging-Best-Practise/#general-best-practice","text":"Engineers should provided enough information engineers understand what was happening when reading the log e.g. Log the client key responsible for the call _logger.LogInformation(\"Create Controlled Vocabulary '{VocabName}' for client {clientKey}\", request.ControlledVocabulary.Name, clientKey); Engineers should use the correct log level when generating logs (see here)","title":"General Best Practice"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/","text":"[[ TOC ]] Unit Tests \u00b6 Unit testing is a fundamental tool in every engineer's toolbox. Unit tests not only help us test our code, they encourage good design practices, reduce the chances of bugs reaching production, and can even serve as examples or documentation on how code functions. Properly written unit tests can also improve engineer efficiency. It is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the development time for every feature. So why should we write them? Unit tests should: reduce costs by catching bugs earlier and preventing regressions increase engineer confidence in changes speed up the engineer inner loop act as documentation as code Unit tests should also be very predictable (i.e. any failures should indicate broken code) and very fast (unit testing a ) Integration tests \u00b6 Integration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behaviour between two or more units of code. Code Coverage \u00b6 The easiest way of quantitatively measuring the quality of automated tests is by examining code coverage. Whilst a high code coverage percentage does not necessarily mean that the tests are of a high quality, it is certainly true that a low code coverage is indicative of there not being enough automated tests! Note that we are more interested in branch coverage than line coverage. There are two ways that our repositories measure and report on code coverage: By generating an HTML report By checking that committed code meets a minimum coverage in GitHub Actions Note : if the repository does not contain the tools to measure code coverage, follow this guide for .NET repositories to add it. The HTML report can be very simply generated by selecting the Run Code Coverage item from the Tools menu in Visual Studio. This will take a few moments runs the tests and generates the report and displays it in the Visual Studio window. The report gives a detailed breakdown of test coverage by class. Clicking through to a class will then break this down by method and individual line. The report can then be used to: Give an indication of parts of the code that are not tested (and thus need new tests or test cases writing) A methods having a high number of branches (or high cyclometric complexity) is potentially and indication that the code in the method should be broken down or split out into other classes Provide the current code coverage The .github/workflows/tests.yml workflow definition file contains the step to validate code coverage, and sets the minimum coverage level for the repository. Code Coverage Requirements \u00b6 As part of our Quality Standards, we require that around 80% of our code is covered by automated testing. When working on a repository, an engineer should ensure that any new code is also covered by new automated tests to maintain or increase this coverage. Existing repositories may not hit this 80% coverage threshold, so expecting to meet it when making a change to an existing repository may not be realistic. When committing code, an engineer should consult the latest coverage report to get the latest branch coverage, and update the BRANCH_THRESHOLD variable in the tests.yml .","title":"Unit & Integration Testing"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#unit-tests","text":"Unit testing is a fundamental tool in every engineer's toolbox. Unit tests not only help us test our code, they encourage good design practices, reduce the chances of bugs reaching production, and can even serve as examples or documentation on how code functions. Properly written unit tests can also improve engineer efficiency. It is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the development time for every feature. So why should we write them? Unit tests should: reduce costs by catching bugs earlier and preventing regressions increase engineer confidence in changes speed up the engineer inner loop act as documentation as code Unit tests should also be very predictable (i.e. any failures should indicate broken code) and very fast (unit testing a )","title":"Unit Tests"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#integration-tests","text":"Integration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behaviour between two or more units of code.","title":"Integration tests"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#code-coverage","text":"The easiest way of quantitatively measuring the quality of automated tests is by examining code coverage. Whilst a high code coverage percentage does not necessarily mean that the tests are of a high quality, it is certainly true that a low code coverage is indicative of there not being enough automated tests! Note that we are more interested in branch coverage than line coverage. There are two ways that our repositories measure and report on code coverage: By generating an HTML report By checking that committed code meets a minimum coverage in GitHub Actions Note : if the repository does not contain the tools to measure code coverage, follow this guide for .NET repositories to add it. The HTML report can be very simply generated by selecting the Run Code Coverage item from the Tools menu in Visual Studio. This will take a few moments runs the tests and generates the report and displays it in the Visual Studio window. The report gives a detailed breakdown of test coverage by class. Clicking through to a class will then break this down by method and individual line. The report can then be used to: Give an indication of parts of the code that are not tested (and thus need new tests or test cases writing) A methods having a high number of branches (or high cyclometric complexity) is potentially and indication that the code in the method should be broken down or split out into other classes Provide the current code coverage The .github/workflows/tests.yml workflow definition file contains the step to validate code coverage, and sets the minimum coverage level for the repository.","title":"Code Coverage"},{"location":"6.-Engineering/Quality-Standards/Unit-%26-Integration-Testing/#code-coverage-requirements","text":"As part of our Quality Standards, we require that around 80% of our code is covered by automated testing. When working on a repository, an engineer should ensure that any new code is also covered by new automated tests to maintain or increase this coverage. Existing repositories may not hit this 80% coverage threshold, so expecting to meet it when making a change to an existing repository may not be realistic. When committing code, an engineer should consult the latest coverage report to get the latest branch coverage, and update the BRANCH_THRESHOLD variable in the tests.yml .","title":"Code Coverage Requirements"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/","text":"Who is this for? \u00b6 Outlines the current source control techniques for our codebases, referred to as our \"branching strategy\". This is targeted at any member of the team who performs work against any of the codebases, including code changes and reviews. Source Control \u00b6 Code is managed and stored using Git and GitHub. Branching Strategy \u00b6 IMPORTANT NOTE: This strategy applies to all repositions with the exception of any shared code repositories which generate NuGet packages which are dependencies of multiple other repos/solutions. Please see the specific Branching & Versioning Shared Repositories documentation. Branch Key Purpose Naming Main Reflection of production environments. At any point you should be able to branch from Main absolutely confident that it will match what is main Release A release branch is used when EITHER of following conditions are met: 1. multiple features are being released together 2. BAT is required release/{Release-Name} Feature Contains all the changes required for a specific feature or bug, only. Note that the term \"feature branch is used as it is common practice, but it is synonymous with a backlog item - an independent, releasable, valuable product increment feature/{Feature-Name} OR bug/{Bug-Name} Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/[feature|bug]/{Feature Name} Documentation Contains changes to any documentation within the codebase where the changes to the documentation aren't a result of a code change document/{Change-Name} Strategy Basics \u00b6 When work is started on a feature a Feature branch is created from the Main branch. Each engineer who needs to make changed to that branch must create their own Work branch, against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. If multiple engineers are working concurrently on a single feature, and their work is interdependent (i.e. could not be usefully tested independently), each should use their own Work branch and one Work branch should be merged into another, via a pull request (PR) . When the engineer has completed their work, and it is ready for test, a PR can be submitted to merge it into the Feature branch. The PR should be actioned by another member of the team, who should perform a peer review at this time. Changes required should be performed by the original engineer against the merging Work branch, but any other member of the team should do this in their absence. When a PR is completed and the Feature branch has been updated, a build will be automatically kicked off and a releasable package created. This can then be deployed into any available QA environment for testing or PO review. When a feature has passed testing and PO review, the Feature branch can be merged into either a Release branch, the the Main branch, by performing a PR : Release Branches: > Do this if: the feature will be released alongside other features, or is required to undergo BAT. TBC- A Test Engineer will review and assess the PR to ensure that only the correct features are included for a release A new Release branch is created by branching from the Main branch if one does not already exists. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch When a merge into the Release branch is completed, a build will again be started and a releasable package will be created. This can then be deployed into any BAT environment for PO signoff and Business Acceptance Testing. A release plan should be created (or updated), including the package ID. When BAT is completed, the package can be deployed to production environments. Once the release to production environments is complete, the Release branch can be merged into Main via a PR. Main Branch Do this if: the feature is released independently and does not require BAT. Merge to Main only occurs after the changes are successfully released into the production environments. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch. A release plan should be created, including the package ID. Once the release to production environments is complete, the Feature branch can be merged into Main via a PR. Once Main has been updated, communicate the change to all Platform Development teams so that any WIP branches can be updated and regression tests can be performed. Non-Functional Changes \u00b6 If you are deploying non-functional changes (such as documentation or linting), then merging from your work branch to main is fine. This should only be done if the changes are non-functional, if any functional code is changed, your work branch must be merged into a feature branch. // TODO: include images of each step and overall map view Creating a Feature Branch \u00b6 Creating a Release Branch \u00b6 Merging and Pull Requests \u00b6 Merging Upstream \u00b6 Upstream merges occur when changes need to pulled into a parent branch. When this is done, a Pull Request (PR) must be submitted and a peer review performed by another member of the team. You should only merge good work upstream. I.e. it should already be known to pass all quality standards and tests. Merging Downstream \u00b6 Downstream mergers occur when changes need to be pulled from a parent branch into a child branch. For example, a release has occurred while a feature is being worked on. In this case the master branch has changed since the feature branch was created, so the feature branch must be updated with the changes. Any work branches relating to that feature branch would also need to be updated. These changes should be merged and tested prior to attempting a merge up to the parent. Deploying Packages \u00b6 Tooling \u00b6 // TODO: Recommended source control tooling such as Source Tree/ VS Code Plugins etc // TODO: High-level GitHub overview Related Training \u00b6 You should have a good understanding of Git principles. Our branching strategy details the usual activities, but you may need to perform more advanced git commands occasionally. Pluralsight \u00b6 Course: Managing Source Code with Git This course contains a series of training videos from beginner to advanced user need. Git Documentation \u00b6 The reference documentation can provide quick information on commands and their purpose. and there is an advanced user book Pro Git available online as well","title":"Source Control, Versioning & Branching Strategy"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#who-is-this-for","text":"Outlines the current source control techniques for our codebases, referred to as our \"branching strategy\". This is targeted at any member of the team who performs work against any of the codebases, including code changes and reviews.","title":"Who is this for?"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#source-control","text":"Code is managed and stored using Git and GitHub.","title":"Source Control"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#branching-strategy","text":"IMPORTANT NOTE: This strategy applies to all repositions with the exception of any shared code repositories which generate NuGet packages which are dependencies of multiple other repos/solutions. Please see the specific Branching & Versioning Shared Repositories documentation. Branch Key Purpose Naming Main Reflection of production environments. At any point you should be able to branch from Main absolutely confident that it will match what is main Release A release branch is used when EITHER of following conditions are met: 1. multiple features are being released together 2. BAT is required release/{Release-Name} Feature Contains all the changes required for a specific feature or bug, only. Note that the term \"feature branch is used as it is common practice, but it is synonymous with a backlog item - an independent, releasable, valuable product increment feature/{Feature-Name} OR bug/{Bug-Name} Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/[feature|bug]/{Feature Name} Documentation Contains changes to any documentation within the codebase where the changes to the documentation aren't a result of a code change document/{Change-Name}","title":"Branching Strategy"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#strategy-basics","text":"When work is started on a feature a Feature branch is created from the Main branch. Each engineer who needs to make changed to that branch must create their own Work branch, against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. If multiple engineers are working concurrently on a single feature, and their work is interdependent (i.e. could not be usefully tested independently), each should use their own Work branch and one Work branch should be merged into another, via a pull request (PR) . When the engineer has completed their work, and it is ready for test, a PR can be submitted to merge it into the Feature branch. The PR should be actioned by another member of the team, who should perform a peer review at this time. Changes required should be performed by the original engineer against the merging Work branch, but any other member of the team should do this in their absence. When a PR is completed and the Feature branch has been updated, a build will be automatically kicked off and a releasable package created. This can then be deployed into any available QA environment for testing or PO review. When a feature has passed testing and PO review, the Feature branch can be merged into either a Release branch, the the Main branch, by performing a PR : Release Branches: > Do this if: the feature will be released alongside other features, or is required to undergo BAT. TBC- A Test Engineer will review and assess the PR to ensure that only the correct features are included for a release A new Release branch is created by branching from the Main branch if one does not already exists. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch When a merge into the Release branch is completed, a build will again be started and a releasable package will be created. This can then be deployed into any BAT environment for PO signoff and Business Acceptance Testing. A release plan should be created (or updated), including the package ID. When BAT is completed, the package can be deployed to production environments. Once the release to production environments is complete, the Release branch can be merged into Main via a PR. Main Branch Do this if: the feature is released independently and does not require BAT. Merge to Main only occurs after the changes are successfully released into the production environments. Ensure that the feature branch is up-to-date with the Main branch and regression tested before merging it into the Release branch. A release plan should be created, including the package ID. Once the release to production environments is complete, the Feature branch can be merged into Main via a PR. Once Main has been updated, communicate the change to all Platform Development teams so that any WIP branches can be updated and regression tests can be performed.","title":"Strategy Basics"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#non-functional-changes","text":"If you are deploying non-functional changes (such as documentation or linting), then merging from your work branch to main is fine. This should only be done if the changes are non-functional, if any functional code is changed, your work branch must be merged into a feature branch. // TODO: include images of each step and overall map view","title":"Non-Functional Changes"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#creating-a-feature-branch","text":"","title":"Creating a Feature Branch"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#creating-a-release-branch","text":"","title":"Creating a Release Branch"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#merging-and-pull-requests","text":"","title":"Merging and Pull Requests"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#merging-upstream","text":"Upstream merges occur when changes need to pulled into a parent branch. When this is done, a Pull Request (PR) must be submitted and a peer review performed by another member of the team. You should only merge good work upstream. I.e. it should already be known to pass all quality standards and tests.","title":"Merging Upstream"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#merging-downstream","text":"Downstream mergers occur when changes need to be pulled from a parent branch into a child branch. For example, a release has occurred while a feature is being worked on. In this case the master branch has changed since the feature branch was created, so the feature branch must be updated with the changes. Any work branches relating to that feature branch would also need to be updated. These changes should be merged and tested prior to attempting a merge up to the parent.","title":"Merging Downstream"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#deploying-packages","text":"","title":"Deploying Packages"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#tooling","text":"// TODO: Recommended source control tooling such as Source Tree/ VS Code Plugins etc // TODO: High-level GitHub overview","title":"Tooling"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#related-training","text":"You should have a good understanding of Git principles. Our branching strategy details the usual activities, but you may need to perform more advanced git commands occasionally.","title":"Related Training"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#pluralsight","text":"Course: Managing Source Code with Git This course contains a series of training videos from beginner to advanced user need.","title":"Pluralsight"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/#git-documentation","text":"The reference documentation can provide quick information on commands and their purpose. and there is an advanced user book Pro Git available online as well","title":"Git Documentation"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/","text":"Versioning \u00b6 Our shared code packages use Semantic Versioning (or SemVer ) compatible version numbers. These are of the format MAJOR.MINOR.PATCH , and one should increment different parts the version depending on the type of changes being made: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Our CI workflows will only trigger when you merge a pull request into the main branch, and will not overwrite an existing package with an identical version. In practice this means that you must ensure the version number of a package is update every time you make a pull request. As part of the pull request, you should indicate how the version number is changing, and why. When to update version numbers \u00b6 Supporting new feature development \u00b6 When working on a new feature, the most likely types of updates that need to be made to shared code will be one of the following: Creating new Request and Response classes for microservice communication Updating shared schema definitions (e.g. adding a column to a table) Adding new capabilities to other pieces of shared code In this case you should increment the MINOR version and reset the PATCH to 0. If incremental changes need to be made as part of the same feature .e.g as part of peer review comments, or to address defects raised as part of signing), simply increment the PATCH number for each incremental change that needs to be made. Implementing Fixes \u00b6 When working on a fix of some description (e.g. a fix to an extension method) that should be backwards compatible, simply increment the PATCH number. Breaking Changes or Major Updates \u00b6 Examples of a MAJOR update would include: Wholesale refactoring of the package (e.g. splitting some functionality out into a new package) Framework version upgrades (e.g. updating the package to be compatible with .NET 5) IMPORTANT: in addition to the examples noted above, should any change that would usually be a MINOR or PATCH change break compilation on any solution depending on the package, then you should instead increment MAJOR version. A breaking change can be thought of as any change the requires code that depends upon it to be updated. Updating Packages \u00b6 Once a pull request has been approved and merged into main , any updated packages will be automatically built and published to GitHub Packages, ready to be pulled in by any other repositories that need them. Updating Repositories \u00b6 Once a new version of a package has been created, you need to ensure that the relevant other repositories are updated to use it. Applying MAJOR or MINOR version updates \u00b6 Usually, any update to packages that requires a MAJOR or MINOR version change will be made with one or more repositories in mind. E.g. if creating new Request & Response classes to implement a new endpoint, at least the microservice implementing the new endpoint and one or more other microservice will need to access the new classes. For each repository that needs the new code, update the relevant csproj files within the codebase as follows: <ItemGroup> <PackageReference Include= \"Quartex.Package.A\" Version= \"MAJOR.MINOR.*\" /> </ItemGroup> This will ensure the service is built against the MAJOR.MINOR version, whilst automatically ensuring it is pulling in any fixes and updates. Applying PATCH version updates \u00b6 Since we are using wildcard versioning, there is no need to explicitly update repositories to pull in the latest PATCH number. Branching \u00b6 Source Control \u00b6 Code is managed and stored using Git and GitHub. Strategy Basics \u00b6 Our versioning strategy means that every change produces a unique package version, and dependant repositories can specifically target known versions of each package. This means our branching strategy for shared code can be much simpler than the one used for other repository types. Branch Key Purpose Naming Main Reflects the state of the code that produced the last published package. main Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/{Change Name} Each engineer who needs to make changes to a package must create their own Work branch off Main , against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. When changes to the package have been completed, and a new version of the package built, the engineer should submit a Pull Request back to Main . Follow the pull request instructions Fill in the details requested as part of the template pull request Another member of the team should then perform a peer review on the work Any changes requested as part of the peer review should be committed to the Work branch. When the review is completed (including any agreed changes, as above), the pull request can be merged into Main , and new package versions will be automatically created","title":"Branching & Versioning Shared Code Repositories"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#versioning","text":"Our shared code packages use Semantic Versioning (or SemVer ) compatible version numbers. These are of the format MAJOR.MINOR.PATCH , and one should increment different parts the version depending on the type of changes being made: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Our CI workflows will only trigger when you merge a pull request into the main branch, and will not overwrite an existing package with an identical version. In practice this means that you must ensure the version number of a package is update every time you make a pull request. As part of the pull request, you should indicate how the version number is changing, and why.","title":"Versioning"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#when-to-update-version-numbers","text":"","title":"When to update version numbers"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#supporting-new-feature-development","text":"When working on a new feature, the most likely types of updates that need to be made to shared code will be one of the following: Creating new Request and Response classes for microservice communication Updating shared schema definitions (e.g. adding a column to a table) Adding new capabilities to other pieces of shared code In this case you should increment the MINOR version and reset the PATCH to 0. If incremental changes need to be made as part of the same feature .e.g as part of peer review comments, or to address defects raised as part of signing), simply increment the PATCH number for each incremental change that needs to be made.","title":"Supporting new feature development"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#implementing-fixes","text":"When working on a fix of some description (e.g. a fix to an extension method) that should be backwards compatible, simply increment the PATCH number.","title":"Implementing Fixes"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#breaking-changes-or-major-updates","text":"Examples of a MAJOR update would include: Wholesale refactoring of the package (e.g. splitting some functionality out into a new package) Framework version upgrades (e.g. updating the package to be compatible with .NET 5) IMPORTANT: in addition to the examples noted above, should any change that would usually be a MINOR or PATCH change break compilation on any solution depending on the package, then you should instead increment MAJOR version. A breaking change can be thought of as any change the requires code that depends upon it to be updated.","title":"Breaking Changes or Major Updates"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#updating-packages","text":"Once a pull request has been approved and merged into main , any updated packages will be automatically built and published to GitHub Packages, ready to be pulled in by any other repositories that need them.","title":"Updating Packages"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#updating-repositories","text":"Once a new version of a package has been created, you need to ensure that the relevant other repositories are updated to use it.","title":"Updating Repositories"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#applying-major-or-minor-version-updates","text":"Usually, any update to packages that requires a MAJOR or MINOR version change will be made with one or more repositories in mind. E.g. if creating new Request & Response classes to implement a new endpoint, at least the microservice implementing the new endpoint and one or more other microservice will need to access the new classes. For each repository that needs the new code, update the relevant csproj files within the codebase as follows: <ItemGroup> <PackageReference Include= \"Quartex.Package.A\" Version= \"MAJOR.MINOR.*\" /> </ItemGroup> This will ensure the service is built against the MAJOR.MINOR version, whilst automatically ensuring it is pulling in any fixes and updates.","title":"Applying MAJOR or MINOR version updates"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#applying-patch-version-updates","text":"Since we are using wildcard versioning, there is no need to explicitly update repositories to pull in the latest PATCH number.","title":"Applying PATCH version updates"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#branching","text":"","title":"Branching"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#source-control","text":"Code is managed and stored using Git and GitHub.","title":"Source Control"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-%26-Versioning-Shared-Code-Repositories/#strategy-basics","text":"Our versioning strategy means that every change produces a unique package version, and dependant repositories can specifically target known versions of each package. This means our branching strategy for shared code can be much simpler than the one used for other repository types. Branch Key Purpose Naming Main Reflects the state of the code that produced the last published package. main Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/{Change Name} Each engineer who needs to make changes to a package must create their own Work branch off Main , against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. When changes to the package have been completed, and a new version of the package built, the engineer should submit a Pull Request back to Main . Follow the pull request instructions Fill in the details requested as part of the template pull request Another member of the team should then perform a peer review on the work Any changes requested as part of the peer review should be committed to the Work branch. When the review is completed (including any agreed changes, as above), the pull request can be merged into Main , and new package versions will be automatically created","title":"Strategy Basics"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-Functions-as-a-Service/","text":"Our Functions-as-a-Service (Faas) are deployed and managed on AWS using Serverless , and with that comes the concept of stages ; the ability to create different stacks of the same service. This concept works really well when you need to provide different types of environments for the software development lifecycle of your team or organisation, as it allows you to deploy development code to a development environment using a development stage. Source Control \u00b6 Code is managed and stored using Git and GitHub. Strategy Basics \u00b6 Branch Key Purpose Naming Main Reflects the state of the code that produced the live version of the function. main Stage Used to manage instances of a function for testing and development. stage/{Stage-Name} Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/{Change Name} When work is started on a feature a Stage branch is created from the Main branch. Each engineer who needs to make changes to that branch must create their own Work branch, against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. If multiple engineers are working concurrently on a single feature, and their work is interdependent (i.e. could not be usefully tested independently), each should use their own Work branch and one Work branch should be merged into another, via a pull request (PR) . When the engineer has completed their work, and it is ready for testing, a PR can be submitted to merge it into the Stage branch. The PR should be actioned by another member of the team, who should perform a peer review at this time. Changes required should be performed by the original engineer against the merging Work branch, but any other member of the team should do this in their absence. When a PR is completed and the Stage branch has been updated, a build will be automatically kicked off and the function will be deployed if successful. When a feature has passed testing and PO review, the Stage branch can be merged into the Main branch, by performing a PR . This will trigger a build and deployment to the live version of a function. Once Main has been updated, communicate the change to all Platform Development teams so that any WIP branches can be updated and regression tests can be performed.","title":"Branching for Functions as a Service"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-Functions-as-a-Service/#source-control","text":"Code is managed and stored using Git and GitHub.","title":"Source Control"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Branching-Functions-as-a-Service/#strategy-basics","text":"Branch Key Purpose Naming Main Reflects the state of the code that produced the live version of the function. main Stage Used to manage instances of a function for testing and development. stage/{Stage-Name} Work Contains only the work of an individual within the team. Work is only ever done against a work branch work/{Engineer Name}/{Change Name} When work is started on a feature a Stage branch is created from the Main branch. Each engineer who needs to make changes to that branch must create their own Work branch, against which they can commit all changes. Regular and small commits are recommended, and should be regularly pushed . Each commit must be accompanied by a useful description of the change made. If multiple engineers are working concurrently on a single feature, and their work is interdependent (i.e. could not be usefully tested independently), each should use their own Work branch and one Work branch should be merged into another, via a pull request (PR) . When the engineer has completed their work, and it is ready for testing, a PR can be submitted to merge it into the Stage branch. The PR should be actioned by another member of the team, who should perform a peer review at this time. Changes required should be performed by the original engineer against the merging Work branch, but any other member of the team should do this in their absence. When a PR is completed and the Stage branch has been updated, a build will be automatically kicked off and the function will be deployed if successful. When a feature has passed testing and PO review, the Stage branch can be merged into the Main branch, by performing a PR . This will trigger a build and deployment to the live version of a function. Once Main has been updated, communicate the change to all Platform Development teams so that any WIP branches can be updated and regression tests can be performed.","title":"Strategy Basics"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Pull-Requests/","text":"Pull Requests \u00b6 A pull request (PR) is required when merging a child branch into a parent . NOTE: manually merging or committing directly to a parent branch is not permitted. Firstly because it skips the PR mechanism (and therefore bypasses an important quality control step), and secondly because only a PR will trigger the relevant Continuous Integration workflows and builds. A pull request will ideally not be the first time another engineer has seen the work in progress! Think of it more as a final check; pair programming and ad-hoc discussions on possible approaches and patterns would ideally have already given one or more other engineers in the team an opportunity to see the code and provide feedback. Remember, early feedback means we can spot and rectify mistakes earlier. Doing so later in the process will require more time and effort. Creating a Pull Request \u00b6 When an engineer has completed their work on a child branch, and has committed and pushed it to GitHub, they should visit the GitHub website and issue a pull request. This can be done either on the repository's main page (GitHub will usually prompt a user to issue a pull request if they have recently pushed a child branch), or by navigating to the Pull requests tab and clicking the green New pull request button. When creating the pull request, the engineer submitting their work should make sure to: Ensure the base branch is correct (this should be the parent branch, e.g. a Feature branch when merging in a Work branch when working on a new feature) Ensure the compare branch is correct (this should be the engineer's work branch in most situations) Populate the title with a concise summary of what is being submitted Populate the body with an explanation of their approach to solving the problem, including any patterns used Link the request to the relevant ticket in Azure DevOps by adding AB#123 where 123 is the ID of the ticket If there are PRs in other repos that are related to the same piece of work, link to them in the PR description (see hints, below) This shouldn't need to be too detailed, as ideally this won't be the first time they are seeing the code In the case of a shared code repository, provided the information about version number update (i.e. MAJOR vs MINOR vs PATCH) and why it is appropriate Assign their team as a reviewer NOTE: the final step will automatically assign all other team members, and only one team member needs to approve the PR. The engineer submitting the PR should communicate with the rest of their team to request a review. Reviewing Pull Requests & Providing Feedback \u00b6 The reviewing engineer(s) should get a GitHub notification, and should review the PR at the earliest opportunity, and discuss any questions they may have or suggested changes/fixes that need to be made with the original engineer. Once the review has been completed and the reviewing engineer is happy with the changes (including any agreed changes having been completed and committed), they should approve the changes. The original engineer is then responsible for merging the changes into the parent branch, and monitoring any CI workflows to ensure they complete successfully. Feedback should be giving via comments in the pull request in GitHub. Any member of the team can perform a peer review for anyone else, not just a more senior member! Hints for working with Pull Requests in GitHub \u00b6 You can open a draft Pull Request even before the work is ready for review. When you have finished working on it, you can then click the Ready for Review button on the PR page. Any further commits pushed to the same branch before the Pull Request is merged will be included. This works for both draft and non-draft PRs Beware , the default behaviour is that GitHub will create a PR that merges into the main branch, which is rarely correct. However, it is possible to change the base of a PR before it is merged, simply by clicking the Edit button at the top of the PR page, and then selecting the relevant feature or release branch from the drop down. If you add a link to another GitHub Pull Request (usually from another repo) in the body of your pull request, or within another comment, GitHub will display the status of the linked PR with the main one. Hovering over the link will show a \"card\" containing more details of that other PR.","title":"Pull Requests"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Pull-Requests/#pull-requests","text":"A pull request (PR) is required when merging a child branch into a parent . NOTE: manually merging or committing directly to a parent branch is not permitted. Firstly because it skips the PR mechanism (and therefore bypasses an important quality control step), and secondly because only a PR will trigger the relevant Continuous Integration workflows and builds. A pull request will ideally not be the first time another engineer has seen the work in progress! Think of it more as a final check; pair programming and ad-hoc discussions on possible approaches and patterns would ideally have already given one or more other engineers in the team an opportunity to see the code and provide feedback. Remember, early feedback means we can spot and rectify mistakes earlier. Doing so later in the process will require more time and effort.","title":"Pull Requests"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Pull-Requests/#creating-a-pull-request","text":"When an engineer has completed their work on a child branch, and has committed and pushed it to GitHub, they should visit the GitHub website and issue a pull request. This can be done either on the repository's main page (GitHub will usually prompt a user to issue a pull request if they have recently pushed a child branch), or by navigating to the Pull requests tab and clicking the green New pull request button. When creating the pull request, the engineer submitting their work should make sure to: Ensure the base branch is correct (this should be the parent branch, e.g. a Feature branch when merging in a Work branch when working on a new feature) Ensure the compare branch is correct (this should be the engineer's work branch in most situations) Populate the title with a concise summary of what is being submitted Populate the body with an explanation of their approach to solving the problem, including any patterns used Link the request to the relevant ticket in Azure DevOps by adding AB#123 where 123 is the ID of the ticket If there are PRs in other repos that are related to the same piece of work, link to them in the PR description (see hints, below) This shouldn't need to be too detailed, as ideally this won't be the first time they are seeing the code In the case of a shared code repository, provided the information about version number update (i.e. MAJOR vs MINOR vs PATCH) and why it is appropriate Assign their team as a reviewer NOTE: the final step will automatically assign all other team members, and only one team member needs to approve the PR. The engineer submitting the PR should communicate with the rest of their team to request a review.","title":"Creating a Pull Request"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Pull-Requests/#reviewing-pull-requests-providing-feedback","text":"The reviewing engineer(s) should get a GitHub notification, and should review the PR at the earliest opportunity, and discuss any questions they may have or suggested changes/fixes that need to be made with the original engineer. Once the review has been completed and the reviewing engineer is happy with the changes (including any agreed changes having been completed and committed), they should approve the changes. The original engineer is then responsible for merging the changes into the parent branch, and monitoring any CI workflows to ensure they complete successfully. Feedback should be giving via comments in the pull request in GitHub. Any member of the team can perform a peer review for anyone else, not just a more senior member!","title":"Reviewing Pull Requests &amp; Providing Feedback"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Pull-Requests/#hints-for-working-with-pull-requests-in-github","text":"You can open a draft Pull Request even before the work is ready for review. When you have finished working on it, you can then click the Ready for Review button on the PR page. Any further commits pushed to the same branch before the Pull Request is merged will be included. This works for both draft and non-draft PRs Beware , the default behaviour is that GitHub will create a PR that merges into the main branch, which is rarely correct. However, it is possible to change the base of a PR before it is merged, simply by clicking the Edit button at the top of the PR page, and then selecting the relevant feature or release branch from the drop down. If you add a link to another GitHub Pull Request (usually from another repo) in the body of your pull request, or within another comment, GitHub will display the status of the linked PR with the main one. Hovering over the link will show a \"card\" containing more details of that other PR.","title":"Hints for working with Pull Requests in GitHub"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Versioning-Microservices/","text":"This document has yet to be written. \u00b6 Microservice versioning is scheduled for review in Q4 2021","title":"Versioning Microservices"},{"location":"6.-Engineering/Source-Control%2C-Versioning-%26-Branching-Strategy/Versioning-Microservices/#this-document-has-yet-to-be-written","text":"Microservice versioning is scheduled for review in Q4 2021","title":"This document has yet to be written."},{"location":"6.-Engineering/Test-Engineering/","text":"Who is this for? \u00b6 Outlines the testing standards and strategies to which Test Engineers and Software Engineers must follow. Overview \u00b6 This section contains an overview of testing practices, tools and principles to be used by the team. It is important to note these are guidelines and not rules. Although these guidelines must be followed, there will be scenarios where this is not practical. These are living documents and should be reviewed and updated, where appropriate.","title":"Test Engineering"},{"location":"6.-Engineering/Test-Engineering/#who-is-this-for","text":"Outlines the testing standards and strategies to which Test Engineers and Software Engineers must follow.","title":"Who is this for?"},{"location":"6.-Engineering/Test-Engineering/#overview","text":"This section contains an overview of testing practices, tools and principles to be used by the team. It is important to note these are guidelines and not rules. Although these guidelines must be followed, there will be scenarios where this is not practical. These are living documents and should be reviewed and updated, where appropriate.","title":"Overview"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/","text":"Introduction \u00b6 This document defines the overarching functional test strategy followed at AM to accompany and support the SDLC. Further strategies are available to cover exploratory testing, automated testing etc in more detail. Test Objectives \u00b6 We use both static and dynamic testing as a means for achieving test objectives. Both provide information to improve the Backlog Item under test, and our development and testing processes. Our main testing objectives are to: - Verify documentation in order to prevent defects - Detect defects existing in the software under test - Determine the solution satisfies our Stakeholders needs - Demonstrate the software is fit for purpose - Gain confidence in and provide information about the level of quality Test Approach \u00b6 Test Levels - Responsibility & Tools \u00b6 This section details the test levels frequently applied and who has responsibility for performing this testing\u202f Test Level Objective Tools Used Responsibility Unit Testing Focus on individual components Delivery Team Integration Testing Focus on interactions between components or systems Delivery Team System Testing Focus on behaviour and capability of the whole system are as designed and specified TestPad / Playwright / BrowserStack / Azure DevOps Delivery Team Business Acceptance Testing (BAT) Focus on validating the fitness of use of the system in a real or simulated operational environment Stakeholders Test Types \u00b6 Static Testing - Testing at specification level without execution of the software e.g. reviewing Backlog Items Functional Testing - Functional testing is based on the analysis of the behaviour of the component / system i.e. \u2018what it does\u2019 - Ensures the product meets our stakeholders' requirements and doesn\u2019t have any major bugs Confirmation Testing - When a test fails and a defect reported, we can expect a new version of the software that has had the defect fixed. In this case, we will need to execute the test again to confirm the defect has indeed been resolved Regression Testing - Verify that modifications in the software or environment have not caused unintended adverse side effects and that the system still meets its requirements Smoke Testing - These are basic tests that are quick to execute with the goal of ensuring the major features of the system are working as expected - Smoke tests are useful following a new build/release: - To verify a successful build/release - To decide whether more expensive testing can commence Business Acceptance Testing (BAT) - This tends to be the last phase of testing. During BAT, our Stakeholders test to make sure the software can handle required tasks in real-world scenarios, according to requirements Test Design Techniques \u00b6 There are many different types of software testing techniques, each with its own strengths and weaknesses. At AM, we are not limited to which technique is used and it is therefore left to the judgement of the Engineer. Following is a list of common test techniques: Specification Based - Equivalence Partitioning - Boundary Value Analysis - Decision Tables - Pairwise Experience Based - Error Guessing - Checklist-Based - Exploratory Supported Browsers & Devices \u00b6 Please refer to the Supported Browsers, Devices & Operating Systems documentation Test Process \u00b6 Readying \u00b6 Consider how the team will approach testing the backlog Item and note a high-level strategy Planning \u00b6 In the planning phase, we aim to: - Identify test scenarios\u202f - Define the test approach, including: - Assess which scenarios can be automated\u202f - Define approach to manual testing - Identify test data required\u202f Execution \u00b6 Once development of the Backlog Item is available on a test environment, the test approach is executed. Further details on assessing results can be found in the relevant test strategy (e.g. Manual Test Strategy / Automation Test Strategy) Exit Criteria \u00b6 Exit Criteria is discussed on a per Backlog Item basis. Points to consider: - Number of Test Scenarios executed\u202f - Desired and sufficient coverage of the requirements - Software delivered has not regressed - Identified defects are addressed - Any high priority/severity defects outstanding\u202f Defect/Bug Management \u00b6 Defects are raised in Azure DevOps as a child task of the relevant Backlog Item. Production Bugs are raised in Azure DevOps as a Backlog Item. As a minimum, the following data is logged: - Test Data - Steps to replicate the defect or bug - Expected Results - Actual Results - Test Evidence, where possible Release Control \u00b6 Please refer to the Release Management documentation //TO DO Other strategies \u00b6 The following strategies are an extension to this Functional Test Strategy: - Manual Testing Strategy //TO DO - Automation Testing Strategy //TO DO - Testing Package Dependencies //TO DO","title":"Functional Test Strategy"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#introduction","text":"This document defines the overarching functional test strategy followed at AM to accompany and support the SDLC. Further strategies are available to cover exploratory testing, automated testing etc in more detail.","title":"Introduction"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#test-objectives","text":"We use both static and dynamic testing as a means for achieving test objectives. Both provide information to improve the Backlog Item under test, and our development and testing processes. Our main testing objectives are to: - Verify documentation in order to prevent defects - Detect defects existing in the software under test - Determine the solution satisfies our Stakeholders needs - Demonstrate the software is fit for purpose - Gain confidence in and provide information about the level of quality","title":"Test Objectives"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#test-approach","text":"","title":"Test Approach"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#test-levels-responsibility-tools","text":"This section details the test levels frequently applied and who has responsibility for performing this testing\u202f Test Level Objective Tools Used Responsibility Unit Testing Focus on individual components Delivery Team Integration Testing Focus on interactions between components or systems Delivery Team System Testing Focus on behaviour and capability of the whole system are as designed and specified TestPad / Playwright / BrowserStack / Azure DevOps Delivery Team Business Acceptance Testing (BAT) Focus on validating the fitness of use of the system in a real or simulated operational environment Stakeholders","title":"Test Levels - Responsibility &amp; Tools"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#test-types","text":"Static Testing - Testing at specification level without execution of the software e.g. reviewing Backlog Items Functional Testing - Functional testing is based on the analysis of the behaviour of the component / system i.e. \u2018what it does\u2019 - Ensures the product meets our stakeholders' requirements and doesn\u2019t have any major bugs Confirmation Testing - When a test fails and a defect reported, we can expect a new version of the software that has had the defect fixed. In this case, we will need to execute the test again to confirm the defect has indeed been resolved Regression Testing - Verify that modifications in the software or environment have not caused unintended adverse side effects and that the system still meets its requirements Smoke Testing - These are basic tests that are quick to execute with the goal of ensuring the major features of the system are working as expected - Smoke tests are useful following a new build/release: - To verify a successful build/release - To decide whether more expensive testing can commence Business Acceptance Testing (BAT) - This tends to be the last phase of testing. During BAT, our Stakeholders test to make sure the software can handle required tasks in real-world scenarios, according to requirements","title":"Test Types"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#test-design-techniques","text":"There are many different types of software testing techniques, each with its own strengths and weaknesses. At AM, we are not limited to which technique is used and it is therefore left to the judgement of the Engineer. Following is a list of common test techniques: Specification Based - Equivalence Partitioning - Boundary Value Analysis - Decision Tables - Pairwise Experience Based - Error Guessing - Checklist-Based - Exploratory","title":"Test Design Techniques"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#supported-browsers-devices","text":"Please refer to the Supported Browsers, Devices & Operating Systems documentation","title":"Supported Browsers &amp; Devices"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#test-process","text":"","title":"Test Process"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#readying","text":"Consider how the team will approach testing the backlog Item and note a high-level strategy","title":"Readying"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#planning","text":"In the planning phase, we aim to: - Identify test scenarios\u202f - Define the test approach, including: - Assess which scenarios can be automated\u202f - Define approach to manual testing - Identify test data required","title":"Planning"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#execution","text":"Once development of the Backlog Item is available on a test environment, the test approach is executed. Further details on assessing results can be found in the relevant test strategy (e.g. Manual Test Strategy / Automation Test Strategy)","title":"Execution"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#exit-criteria","text":"Exit Criteria is discussed on a per Backlog Item basis. Points to consider: - Number of Test Scenarios executed\u202f - Desired and sufficient coverage of the requirements - Software delivered has not regressed - Identified defects are addressed - Any high priority/severity defects outstanding","title":"Exit Criteria"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#defectbug-management","text":"Defects are raised in Azure DevOps as a child task of the relevant Backlog Item. Production Bugs are raised in Azure DevOps as a Backlog Item. As a minimum, the following data is logged: - Test Data - Steps to replicate the defect or bug - Expected Results - Actual Results - Test Evidence, where possible","title":"Defect/Bug Management"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#release-control","text":"Please refer to the Release Management documentation //TO DO","title":"Release Control"},{"location":"6.-Engineering/Test-Engineering/Functional%20Test%20Strategy/#other-strategies","text":"The following strategies are an extension to this Functional Test Strategy: - Manual Testing Strategy //TO DO - Automation Testing Strategy //TO DO - Testing Package Dependencies //TO DO","title":"Other strategies"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/","text":"Who is this for? \u00b6 This page documents the browsers, devices and operating systems we support and therefore test against. These must be considered when designing, coding, as well as testing AM applications. Introduction \u00b6 With such a wide range of browsers, devices and operating systems available, cross browser testing has become a crucial part of the Software Development Lifecycle. It is important to compare a web applications functionality and design across multiple browsers and platforms to ensure consistent behaviour, functionality and user experience. Styling and effects such as mouse-over, pop-ups and fonts can vary significantly across browsers as well as features such as page navigation. Whilst some of these differences can be impossible to resolve, by cross-browser testing we can ensure our users are able to access all content and execute all functionality without any major issues. Testing Considerations \u00b6 As there are so many combinations of screen sizes, devices, operating systems and browsers to choose from, and limited time and resource to test, it is important that we establish the intended range of configurations that we support to ensure the best coverage of our users. Ideally, it is best to view and test these applications on physical devices with the same configuration as the end user. However, in practice this is not always possible and therefore we use BrowserStack. BrowserStack is a cloud-based cross-browser testing tool that enables us to test our websites across various browsers on different operating systems and mobile devices without having to install virtual machines or emulators. What we support \u00b6 Evaluating the analytics available to us, it has been agreed to support the following as a minimum: Quartex Applications \u00b6 DAM & CMS \u00b6 Browsers: * Chrome\u202f/ Edge * Firefox\u202f * Safari\u202f * IE ( until end-of-life 15/06/22? ) Operating Systems: * Windows * MacOS Devices: * Desktop * Laptop Published Sites \u00b6 Browsers: * Chrome\u202f/ Edge * Firefox\u202f * Safari\u202f * IE ( until end-of-life 15/06/22? ) Operating Systems: * Windows * Android * MacOS * iOS Devices: * Desktop * Laptop * Tablet * Mobile Phone Other Applicaions \u00b6 Internal Applications \u00b6 This covers applications such as the Admin tool and AM Portal Browsers: * Chrome Operating Systems: * Windows Devices: * Desktop * Laptop Framework & White Label \u00b6 Browsers: * Chrome / Edge * Firefox * Safari * IE11 Operating Systems: * Windows * MacOS Devices: * Desktop * Laptop","title":"Supported Browsers, Devices & Operating Systems"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#who-is-this-for","text":"This page documents the browsers, devices and operating systems we support and therefore test against. These must be considered when designing, coding, as well as testing AM applications.","title":"Who is this for?"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#introduction","text":"With such a wide range of browsers, devices and operating systems available, cross browser testing has become a crucial part of the Software Development Lifecycle. It is important to compare a web applications functionality and design across multiple browsers and platforms to ensure consistent behaviour, functionality and user experience. Styling and effects such as mouse-over, pop-ups and fonts can vary significantly across browsers as well as features such as page navigation. Whilst some of these differences can be impossible to resolve, by cross-browser testing we can ensure our users are able to access all content and execute all functionality without any major issues.","title":"Introduction"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#testing-considerations","text":"As there are so many combinations of screen sizes, devices, operating systems and browsers to choose from, and limited time and resource to test, it is important that we establish the intended range of configurations that we support to ensure the best coverage of our users. Ideally, it is best to view and test these applications on physical devices with the same configuration as the end user. However, in practice this is not always possible and therefore we use BrowserStack. BrowserStack is a cloud-based cross-browser testing tool that enables us to test our websites across various browsers on different operating systems and mobile devices without having to install virtual machines or emulators.","title":"Testing Considerations"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#what-we-support","text":"Evaluating the analytics available to us, it has been agreed to support the following as a minimum:","title":"What we support"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#quartex-applications","text":"","title":"Quartex Applications"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#dam-cms","text":"Browsers: * Chrome\u202f/ Edge * Firefox\u202f * Safari\u202f * IE ( until end-of-life 15/06/22? ) Operating Systems: * Windows * MacOS Devices: * Desktop * Laptop","title":"DAM &amp; CMS"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#published-sites","text":"Browsers: * Chrome\u202f/ Edge * Firefox\u202f * Safari\u202f * IE ( until end-of-life 15/06/22? ) Operating Systems: * Windows * Android * MacOS * iOS Devices: * Desktop * Laptop * Tablet * Mobile Phone","title":"Published Sites"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#other-applicaions","text":"","title":"Other Applicaions"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#internal-applications","text":"This covers applications such as the Admin tool and AM Portal Browsers: * Chrome Operating Systems: * Windows Devices: * Desktop * Laptop","title":"Internal Applications"},{"location":"6.-Engineering/Test-Engineering/Supported-Browsers%2C-Devices-%26-Operating-Systems/#framework-white-label","text":"Browsers: * Chrome / Edge * Firefox * Safari * IE11 Operating Systems: * Windows * MacOS Devices: * Desktop * Laptop","title":"Framework &amp; White Label"},{"location":"7.-Technology-Vision-and-Strategy/","text":"Who is this for? \u00b6 This section describes the way in which we plan ahead to ensure our technology estate will continue to serve the business and product needs in the future. All members of Platform Development will have an interest in ensuring that this approach is fit for purpose, and engineers need to engage with the vision and strategy by offering their expertise and insights to help us define and refine the path ahead Vision \u00b6 Continually improve our effectiveness, by improving our ability to deliver value fast and sustainably, whilst meeting quality, security, stability and performance needs to increase the longevity of our technology while increasing the rate at which we deliver value Guiding Principals \u00b6 All engineers must engage and contribute towards our technology strategy, roadmap and backlog The Tech Leadership team will are here to help drive the strategy forwards, not dictate it Ideas are validated rapidly before being included in the strategy or added to the roadmap Engineers must work collaboratively to ensure roadmap items are prepared for sprints in a timely manner Finding the best approach and solution for the problem is more important than delivering it quickly: a problem solved quickly is not the same as a problem solved well. Contextual Overview \u00b6 Our Technology Vision guides our Tech Strategy. Our Technology Strategy describes what we aim to achieve in mid-to-long term, in line with our guiding vision The Technology Strategy informs our Tech Roadmap , a set of high level outcomes, which are broken down into discrete deliverables to form our Backlog . This sits alongside our Product and Support visions, strategies, roadmaps and backlogs Our departmental mission links them together.","title":"Technology Vision and Strategy"},{"location":"7.-Technology-Vision-and-Strategy/#who-is-this-for","text":"This section describes the way in which we plan ahead to ensure our technology estate will continue to serve the business and product needs in the future. All members of Platform Development will have an interest in ensuring that this approach is fit for purpose, and engineers need to engage with the vision and strategy by offering their expertise and insights to help us define and refine the path ahead","title":"Who is this for?"},{"location":"7.-Technology-Vision-and-Strategy/#vision","text":"Continually improve our effectiveness, by improving our ability to deliver value fast and sustainably, whilst meeting quality, security, stability and performance needs to increase the longevity of our technology while increasing the rate at which we deliver value","title":"Vision"},{"location":"7.-Technology-Vision-and-Strategy/#guiding-principals","text":"All engineers must engage and contribute towards our technology strategy, roadmap and backlog The Tech Leadership team will are here to help drive the strategy forwards, not dictate it Ideas are validated rapidly before being included in the strategy or added to the roadmap Engineers must work collaboratively to ensure roadmap items are prepared for sprints in a timely manner Finding the best approach and solution for the problem is more important than delivering it quickly: a problem solved quickly is not the same as a problem solved well.","title":"Guiding Principals"},{"location":"7.-Technology-Vision-and-Strategy/#contextual-overview","text":"Our Technology Vision guides our Tech Strategy. Our Technology Strategy describes what we aim to achieve in mid-to-long term, in line with our guiding vision The Technology Strategy informs our Tech Roadmap , a set of high level outcomes, which are broken down into discrete deliverables to form our Backlog . This sits alongside our Product and Support visions, strategies, roadmaps and backlogs Our departmental mission links them together.","title":"Contextual Overview"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/","text":"Overview \u00b6 Describe the general purpose of the Tech Roadmap Ideas are validated and added to roadmap -> Roadmap items are broken down into backlog items (describe item types, add examples) Describe who owns and contributes to it and who it is for Discovery \u00b6 Technology Vision and Strategy \u00b6 Describe the purpose of the Vision and strategy and link to it Capturing Ideas and Creating Roadmap Items \u00b6 At any point any member of the platform development team can (and should!) add new ideas to the technology ideation repository. Each of these ideas needs to go through some degree of validation This will depend upon the estimated complexity and scale of the idea Validation is performed using the \"Technology Canvas\" Each item will have a lead assigned Validation means ensuring it is valuable, feasible, and aligned with our vision and strategy Create an Idea item and complete as much of the canvas as possible Ideas are reviewed by the engineering team periodically (practice TBC), during which early prioritisation is performed and a lead is assigned The lead will then continue the validation process and complete the canvas If it is determined not be be valuable, viable, feasible, the reasons for this are captured, and no further work is done Otherwise it will be shared with the Technology Governance Group (TGG), who will evaluate and feedback, offering guidance for further validation. The TGG may also veto. Once the TGG are aligned with the canvas the Lead will estimate, rank, prioritise, and set a delivery target, utilising other members of the team to do this. Then add to the roadmap. (TLDR: any validated idea that has a delivery target, is essentially on the roadmap) The TGG will review the roadmap and feedback on, or adjust priority Guidelines for Technology Governance Group \u00b6 When evaluating canvases, feedback should help them further improve the validation they have performed, rather than directly offering a different opinion This will allow them to improve their ability to validate in the future None the less, the TGG may veto the idea regardless of the conclusions/suggestions made, if it is deemed tangential to the vision and mission The TGG need to ensure they set aside time to support the discovery process The TGG should organise regular sessions to review the roadmap priorities. Definition \u00b6 Similar to definition practice for Product BLIs The lead for the Technical Roadmap Item (TRI) will typically be the same person who was lead for the Idea, but this will not always be possible or appropriate The TRI lead should convene a \"3 Amigos\" group (or an \"N Amigos\" group) to provide input from Technical, Testing, Design, Security (etc) perspectives as appropriate to the TRI TRIs are broken down into one or more backlog items (BLIs), typically these will be \"Technical Improvements\" During the definition phase we explore and define overall solution we are aiming for, then turn this into a number of distinct deliverables At appropriate points within the definition process, the Software Architect should be consulted, who can provide guidance and feedback as to how the solution fits within the overall platform. As with product BLIs, the technical improvement BLIs should contain an overview of the desired technical solution and architectural design. If it is determined that we have insufficient knowledge to proceed, the N Amigos group can create an Investigation BLI to find any unknowns. The investigation should have a well defined outcome, to ensure the result means we can progress with definition thereafter. Each BLI should then be reviewed and estimated Each backlog item does not need to be delivered at the same time! We can add value gradually. This plan should be captured in the parent Roadmap Item. `// TODO Define actual mechanisms for assigning leads, reviewing backlog/ideas repo Define \"ready\" Summarise practice for getting work into sprints i.e. pre-planning Describe the different \"Technical Value Types\"","title":"Strategy and Roadmap"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#overview","text":"Describe the general purpose of the Tech Roadmap Ideas are validated and added to roadmap -> Roadmap items are broken down into backlog items (describe item types, add examples) Describe who owns and contributes to it and who it is for","title":"Overview"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#discovery","text":"","title":"Discovery"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#technology-vision-and-strategy","text":"Describe the purpose of the Vision and strategy and link to it","title":"Technology Vision and Strategy"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#capturing-ideas-and-creating-roadmap-items","text":"At any point any member of the platform development team can (and should!) add new ideas to the technology ideation repository. Each of these ideas needs to go through some degree of validation This will depend upon the estimated complexity and scale of the idea Validation is performed using the \"Technology Canvas\" Each item will have a lead assigned Validation means ensuring it is valuable, feasible, and aligned with our vision and strategy Create an Idea item and complete as much of the canvas as possible Ideas are reviewed by the engineering team periodically (practice TBC), during which early prioritisation is performed and a lead is assigned The lead will then continue the validation process and complete the canvas If it is determined not be be valuable, viable, feasible, the reasons for this are captured, and no further work is done Otherwise it will be shared with the Technology Governance Group (TGG), who will evaluate and feedback, offering guidance for further validation. The TGG may also veto. Once the TGG are aligned with the canvas the Lead will estimate, rank, prioritise, and set a delivery target, utilising other members of the team to do this. Then add to the roadmap. (TLDR: any validated idea that has a delivery target, is essentially on the roadmap) The TGG will review the roadmap and feedback on, or adjust priority","title":"Capturing Ideas and Creating Roadmap Items"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#guidelines-for-technology-governance-group","text":"When evaluating canvases, feedback should help them further improve the validation they have performed, rather than directly offering a different opinion This will allow them to improve their ability to validate in the future None the less, the TGG may veto the idea regardless of the conclusions/suggestions made, if it is deemed tangential to the vision and mission The TGG need to ensure they set aside time to support the discovery process The TGG should organise regular sessions to review the roadmap priorities.","title":"Guidelines for Technology Governance Group"},{"location":"7.-Technology-Vision-and-Strategy/Strategy-and-Roadmap/#definition","text":"Similar to definition practice for Product BLIs The lead for the Technical Roadmap Item (TRI) will typically be the same person who was lead for the Idea, but this will not always be possible or appropriate The TRI lead should convene a \"3 Amigos\" group (or an \"N Amigos\" group) to provide input from Technical, Testing, Design, Security (etc) perspectives as appropriate to the TRI TRIs are broken down into one or more backlog items (BLIs), typically these will be \"Technical Improvements\" During the definition phase we explore and define overall solution we are aiming for, then turn this into a number of distinct deliverables At appropriate points within the definition process, the Software Architect should be consulted, who can provide guidance and feedback as to how the solution fits within the overall platform. As with product BLIs, the technical improvement BLIs should contain an overview of the desired technical solution and architectural design. If it is determined that we have insufficient knowledge to proceed, the N Amigos group can create an Investigation BLI to find any unknowns. The investigation should have a well defined outcome, to ensure the result means we can progress with definition thereafter. Each BLI should then be reviewed and estimated Each backlog item does not need to be delivered at the same time! We can add value gradually. This plan should be captured in the parent Roadmap Item. `// TODO Define actual mechanisms for assigning leads, reviewing backlog/ideas repo Define \"ready\" Summarise practice for getting work into sprints i.e. pre-planning Describe the different \"Technical Value Types\"","title":"Definition"}]}